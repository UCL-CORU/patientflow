{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#patientflow-code-and-explanatory-notebooks-for-predicting-short-term-hospital-bed-capacity-using-real-time-data","title":"PatientFlow: Code and explanatory notebooks for predicting short-term hospital bed capacity using real-time data","text":"<p>Welcome to the PatientFlow repo, which is designed to support hospital bed management through predictive modelling. The repository shows methods for forecasting short-term bed capacity, a crucial aspect of hospital operations that impacts patient care and resource allocation.</p> <p>Please note that you are looking at this repo prior to its first release. It is incomplete.</p>"},{"location":"#objectives","title":"Objectives","text":"<ol> <li>Develop code that was originally written for University College London Hospital (UCLH) into a reusable resource following the principles of Reproducible Analytical Pipelines</li> <li>Share the resource with analysts, bed managers and other interested parties in the NHS and other hospital systems</li> <li>Provide training materials to inform and educate anyone who wishes to adopt a similar approach</li> </ol>"},{"location":"#main-features-of-our-modelling-approach","title":"Main Features of our modelling approach","text":"<ul> <li>User led: This work is the result of close collaboration with operations directors and bed managers in the Coordination Centre, University College London Hospital (UCLH), over four years. What is modelled directly reflects how they work and what is most useful to them.</li> <li>Focused on short-term predictions: We demonstrate the creation and evaluation of predictive models. The output from these models is a prediction of how many beds with be needed by patients within a short time horizon of (say) 8 hours. (Later we plan to add modules that also predict supply and net bed position over the same period.)</li> <li>Assumes real-time data is available: Our focus is on how hospitals can make use of real-time data to make informed decisions on the ground. All the modelling here assumes that a hospital has some capacity to run models using real-time (or near to real-time) data in its electronic health record, even if this data is minimal.</li> </ul>"},{"location":"#main-features-of-this-repository","title":"Main Features of this repository","text":"<ul> <li>Reproducible - We follow the principles of Reproducible Analytical Pipelines, with the aim that the code can be easily adopted in other settings</li> <li>Accessible - All the elements are based on simple techniques and methods in Health Data Science and Operational Research. The narrative in the notebooks is intended to be accessible to someone without any knowledge of programming; it should still be possible to follow the approach. We intend that anyone with some knowledge of Python could understand and adapt the code for their use.</li> <li>Practical: A synthetic dataset, derived from real patient data, is included within this repo in the <code>data-synthetic</code> folder. This can be used to step through the modelling process if you want to run the notebooks yourself. So even if your hospital is not set up to do real-time prediction yet, you can still follow the same steps we took. (Note that, if you use the synthetic dataset, the integrity of relationships between variables is not maintained and you will obtain articifically inflated model performance.) UCLH have agreed we can release an anomymised version of real patient data, but not within the repo. To gain access to this, please contact Dr Zella King, contact details below.</li> <li>Interactive: The repository includes an accompanying set of notebooks with code written on Python, with commentary. If you clone the repo into your own workspace and have an environment within which to run Jupyter notebooks, you will be able to interact with the code and see it running.</li> </ul>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Exploration: Start with the notebooks README to get an outline of the notebooks, and read the patientflow README to understand our intentions for the Python package</li> <li>Installation: Follow the instructions below to set up the environment and install necessary dependencies in your own environment</li> <li>Configuration: Repurpose config.yaml to configure the package to your own data and user requirements</li> </ul>"},{"location":"#about","title":"About","text":"<p>This project was inspired by the py-pi template developed by Tom Monks, and is developed in collaboration with the Centre for Advanced Research Computing, University College London.</p>"},{"location":"#project-team","title":"Project Team","text":"<p>Dr Zella King, Clinical Operational Research Unit (CORU), UCL (zella.king@ucl.ac.uk) Jon Gillham, Institute of Health Informatics, UCL Professor Sonya Crowe, CORU Professor Martin Utley, CORU</p>"},{"location":"#research-software-engineering-contact","title":"Research Software Engineering Contact","text":"<p>Centre for Advanced Research Computing, University College London (arc.collaborations@ucl.ac.uk)</p>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p><code>patientflow</code> requires Python 3.10.</p>"},{"location":"#installation","title":"Installation","text":"<p>patientflow is not yet available on PyPI. To install the latest development version, clone it first (so that you have access to the synthetic data and the notebooks) and then install it.</p> <pre><code>git clone https://github.com/zmek/patientflow.git\ncd patientflow\npip install -e \".[test]\" #this will install the code in test mode\n\n</code></pre> <p>Navigate to the patientflow folder and run tests to confirm that the installation worked correctly. This command will only work from the root repository. (To date, this has only been tested on Linux and Mac OS machines. If you are running Windows, there may be errors we don't know about.)</p> <pre><code>pytest\n</code></pre> <p>If you get errors running the pytest command, there may be other installations needed on your local machine. (We have found copying the error messages into ChatGPT or Claude very helpful for diagnosing and troubleshooting these errors.)</p>"},{"location":"#training-models-with-data-provided","title":"Training models with data provided","text":"<p>The data provided (which is synthetic) can be used to demonstrate training the models. To run training you have two options</p> <ul> <li>step through the notebooks (for this to work you'll either need copy the two csv files from <code>data-synthetic</code>into your <code>data-public</code> folder or contact us for real patient data)</li> <li>run a Python script using following commands (by default this will run with the synthetic data in its current location; you can change the <code>data_folder_name</code> parameter if you have the real data in <code>data-public</code>)</li> </ul> <pre><code>cd src\npython -m patientflow.train --data_folder_name=data-synthetic --uclh=False\n</code></pre> <p>There are two arguments</p> <ul> <li>data_folder_name - specifies where to find the data. This should be in a folder named data-xxx directly below the root of the repository</li> <li>uclh - tells the package whether the data is the original UCLH data (in which case certain additional features available, including the patient's age in years) or not</li> </ul>"},{"location":"#roadmap","title":"Roadmap","text":"<ul> <li> Initial Research</li> <li> Minimum viable product &lt;-- You are Here</li> <li> Alpha Release</li> <li> Feature-Complete Release</li> </ul>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This work was funded by a grant from the UCL Impact Funding. We are grateful to the Information Governance team and the Caldicott Guardian at UCLH for agreeing that we can release real patient data.</p>"},{"location":"LICENSE/","title":"License","text":""},{"location":"LICENSE/#mit-license","title":"MIT License","text":"<p>Copyright (c) 2024 Zella King</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p>"},{"location":"api/","title":"API reference","text":"<p>Introductory text here</p> <p>test_package</p> <p>Part of a repo containing boilerplate code for publishing on PyPi.</p>"},{"location":"api/#patientflow.aggregate","title":"<code>aggregate</code>","text":"<p>Aggregate Prediction From Patient-Level Probababilities</p> <p>This submodule provides functions to generate a probability distribution, based on inputs that are patient-level probabilities. The module uses symbolic mathematics to build and manipulate expressions dynamically, facilitating the computation of aggregate probabilities.</p> Dependencies <ul> <li>numpy: Used for array and numerical operations.</li> <li>pandas: Utilized for handling data structures like DataFrames, enabling data manipulation and analysis.</li> <li>sympy: A Python library for symbolic mathematics, used here to dynamically create and manipulate symbolic expressions, particularly for the calculation of probabilities.</li> </ul> <p>Functions: - create_symbols(n): Generates symbolic variables. - compute_core_expression(ri, s): Computes a symbolic expression involving both symbols and constants. - build_expression(syms, n): Constructs a cumulative product of symbolic expressions. - expression_subs(expression, n, predictions): Substitutes numerical values into a symbolic expression. - return_coeff(expression, i): Extracts coefficients from expanded symbolic expressions. - model_input_to_pred_proba(model_input, model): Converts model input data into predicted probabilities. - pred_proba_to_agg_predicted(predictions_proba, weights): Aggregates probability predictions . - get_prob_dist_for_prediction_moment(X_test, model, weights, y_test, inference_time): Calculates predicted and (if not inference time) observed values for a specific date. - get_prob_dist(snapshots_dict, X_test, y_test, model, weights): Computes probability distributions for multiple snapshot dates.</p> <p>These functions can work with any model object as long as it provides the predict_proba method. This icludes libraries (like scikit-learn, TensorFlow, or PyTorch), which generally offer this method</p> Example Usage Note <p>This module is designed to be generic and can be adapted to various domains where aggregate prediction is applicable.</p>"},{"location":"api/#patientflow.aggregate--assuming-a-predictive-model-and-test-data-are-available","title":"Assuming a predictive model and test data are available","text":"<p>snapshot_dates = ['2023-01-01', '2023-01-02'] predicted_distribution = get_prob_dist(snapshot_dates, dataset, X_test, y_test, model) print(predicted_distribution)</p>"},{"location":"api/#patientflow.aggregate.build_expression","title":"<code>build_expression(syms, n)</code>","text":"<p>Construct a cumulative product expression by combining individual symbolic expressions.</p>"},{"location":"api/#patientflow.aggregate.build_expression--parameters","title":"Parameters","text":"<p>syms : iterable     Iterable containing symbols to use in the expressions. n : int     The number of terms to include in the cumulative product.</p>"},{"location":"api/#patientflow.aggregate.build_expression--returns","title":"Returns","text":"<p>Expr     The cumulative product of the expressions.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def build_expression(syms, n):\n    \"\"\"\n    Construct a cumulative product expression by combining individual symbolic expressions.\n\n    Parameters\n    ----------\n    syms : iterable\n        Iterable containing symbols to use in the expressions.\n    n : int\n        The number of terms to include in the cumulative product.\n\n    Returns\n    -------\n    Expr\n        The cumulative product of the expressions.\n\n    \"\"\"\n    s = sym.Symbol(\"s\")\n    expression = 1\n    for i in range(n):\n        expression *= compute_core_expression(syms[i], s)\n    return expression\n</code></pre>"},{"location":"api/#patientflow.aggregate.compute_core_expression","title":"<code>compute_core_expression(ri, s)</code>","text":"<p>Compute a symbolic expression involving a basic mathematical operation with a symbol and a constant.</p>"},{"location":"api/#patientflow.aggregate.compute_core_expression--parameters","title":"Parameters","text":"<p>ri : float     The constant value to substitute into the expression. s : Symbol     The symbolic object used in the expression.</p>"},{"location":"api/#patientflow.aggregate.compute_core_expression--returns","title":"Returns","text":"<p>Expr     The symbolic expression after substitution.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def compute_core_expression(ri, s):\n    \"\"\"\n    Compute a symbolic expression involving a basic mathematical operation with a symbol and a constant.\n\n    Parameters\n    ----------\n    ri : float\n        The constant value to substitute into the expression.\n    s : Symbol\n        The symbolic object used in the expression.\n\n    Returns\n    -------\n    Expr\n        The symbolic expression after substitution.\n\n    \"\"\"\n    r = sym.Symbol(\"r\")\n    core_expression = (1 - r) + r * s\n    return core_expression.subs({r: ri})\n</code></pre>"},{"location":"api/#patientflow.aggregate.create_symbols","title":"<code>create_symbols(n)</code>","text":"<p>Generate a sequence of symbolic objects intended for use in mathematical expressions.</p>"},{"location":"api/#patientflow.aggregate.create_symbols--parameters","title":"Parameters","text":"<p>n : int     Number of symbols to create.</p>"},{"location":"api/#patientflow.aggregate.create_symbols--returns","title":"Returns","text":"<p>tuple     A tuple containing the generated symbolic objects.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def create_symbols(n):\n    \"\"\"\n    Generate a sequence of symbolic objects intended for use in mathematical expressions.\n\n    Parameters\n    ----------\n    n : int\n        Number of symbols to create.\n\n    Returns\n    -------\n    tuple\n        A tuple containing the generated symbolic objects.\n\n    \"\"\"\n    return symbols(f\"r0:{n}\")\n</code></pre>"},{"location":"api/#patientflow.aggregate.expression_subs","title":"<code>expression_subs(expression, n, predictions)</code>","text":"<p>Substitute values into a symbolic expression based on a mapping from symbols to predictions.</p>"},{"location":"api/#patientflow.aggregate.expression_subs--parameters","title":"Parameters","text":"<p>expression : Expr     The symbolic expression to perform substitution on. n : int     Number of symbols and corresponding predictions. predictions : list     List of numerical predictions to substitute.</p>"},{"location":"api/#patientflow.aggregate.expression_subs--returns","title":"Returns","text":"<p>Expr     The expression after performing the substitution.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def expression_subs(expression, n, predictions):\n    \"\"\"\n    Substitute values into a symbolic expression based on a mapping from symbols to predictions.\n\n    Parameters\n    ----------\n    expression : Expr\n        The symbolic expression to perform substitution on.\n    n : int\n        Number of symbols and corresponding predictions.\n    predictions : list\n        List of numerical predictions to substitute.\n\n    Returns\n    -------\n    Expr\n        The expression after performing the substitution.\n\n    \"\"\"\n    syms = create_symbols(n)\n    substitution = dict(zip(syms, predictions))\n    return expression.subs(substitution)\n</code></pre>"},{"location":"api/#patientflow.aggregate.get_prob_dist","title":"<code>get_prob_dist(snapshots_dict, X_test, y_test, model, weights=None)</code>","text":"<p>Calculate probability distributions for each snapshot date based on given model predictions.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--parameters","title":"Parameters","text":"<p>snapshots_dict : dict     A dictionary mapping snapshot dates (as datetime objects) to indices in <code>X_test</code> and <code>y_test</code>     that correspond to the snapshots to be tested for each date. X_test : pandas.DataFrame     A DataFrame containing the test features for prediction. y_test : pandas.Series     A Series containing the true outcome values corresponding to the test features in <code>X_test</code>. model : any     A predictive model object with a <code>predict_proba</code> method that takes features from <code>X_test</code> and     optionally weights, and returns a probability distribution over possible outcomes. weights : pandas.Series, optional     A Series containing weights for the test data points, which may influence the prediction,     by default None. If provided, the weights should be indexed similarly to <code>X_test</code> and <code>y_test</code>.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--returns","title":"Returns","text":"<p>dict     A dictionary where each key is a snapshot date and each value is the resulting probability     distribution for that date, obtained by applying the model on the corresponding test snapshots.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist--notes","title":"Notes","text":"<ul> <li>The function asserts that the length of the test features and outcomes are equal for each   snapshot before proceeding with predictions.</li> <li>It notifies the user of progress in processing snapshot dates, especially if there are more   than 10 snapshot dates.</li> </ul> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def get_prob_dist(snapshots_dict, X_test, y_test, model, weights=None):\n    \"\"\"\n    Calculate probability distributions for each snapshot date based on given model predictions.\n\n    Parameters\n    ----------\n    snapshots_dict : dict\n        A dictionary mapping snapshot dates (as datetime objects) to indices in `X_test` and `y_test`\n        that correspond to the snapshots to be tested for each date.\n    X_test : pandas.DataFrame\n        A DataFrame containing the test features for prediction.\n    y_test : pandas.Series\n        A Series containing the true outcome values corresponding to the test features in `X_test`.\n    model : any\n        A predictive model object with a `predict_proba` method that takes features from `X_test` and\n        optionally weights, and returns a probability distribution over possible outcomes.\n    weights : pandas.Series, optional\n        A Series containing weights for the test data points, which may influence the prediction,\n        by default None. If provided, the weights should be indexed similarly to `X_test` and `y_test`.\n\n    Returns\n    -------\n    dict\n        A dictionary where each key is a snapshot date and each value is the resulting probability\n        distribution for that date, obtained by applying the model on the corresponding test snapshots.\n\n    Notes\n    -----\n    - The function asserts that the length of the test features and outcomes are equal for each\n      snapshot before proceeding with predictions.\n    - It notifies the user of progress in processing snapshot dates, especially if there are more\n      than 10 snapshot dates.\n\n    \"\"\"\n    prob_dist_dict = {}\n    print(\n        f\"Calculating probability distributions for {len(snapshots_dict)} snapshot dates\"\n    )\n\n    if len(snapshots_dict) &gt; 10:\n        print(\"This may take a minute or more\")\n\n    # Initialize a counter for notifying the user every 10 snapshot dates processed\n    count = 0\n\n    for dt, snapshots_to_include in snapshots_dict.items():\n        if len(snapshots_to_include) == 0:\n            # Create an empty dictionary for the current snapshot date\n            prob_dist_dict[dt] = {\n                \"agg_predicted\": pd.DataFrame({\"agg_proba\": [1]}, index=[0]),\n                \"agg_observed\": 0,\n            }\n        else:\n            # Ensure the lengths of test features and outcomes are equal\n            assert len(X_test.loc[snapshots_to_include]) == len(\n                y_test.loc[snapshots_to_include]\n            ), \"Mismatch in lengths of X_test and y_test snapshots.\"\n\n            if weights is None:\n                prediction_moment_weights = None\n            else:\n                prediction_moment_weights = weights.loc[snapshots_to_include].values\n\n            # Compute the predicted and observed valuesfor the current snapshot date\n            prob_dist_dict[dt] = get_prob_dist_for_prediction_moment(\n                X_test=X_test.loc[snapshots_to_include],\n                y_test=y_test.loc[snapshots_to_include],\n                model=model,\n                weights=prediction_moment_weights,\n            )\n\n        # Increment the counter and notify the user every 10 snapshot dates processed\n        count += 1\n        if count % 10 == 0 and count != len(snapshots_dict):\n            print(f\"Processed {count} snapshot dates\")\n\n    print(f\"Processed {len(snapshots_dict)} snapshot dates\")\n\n    return prob_dist_dict\n</code></pre>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment","title":"<code>get_prob_dist_for_prediction_moment(X_test, model, weights=None, inference_time=False, y_test=None)</code>","text":"<p>Calculate both predicted distributions and observed values for a given date using test data.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--parameters","title":"Parameters","text":"<p>X_test : array-like     Test features for a specific snapshot date. model : object     A predictive model which should provide a <code>predict_proba</code> method. weights : array-like, optional     Weights to apply to the predictions for aggregate calculation. inference_time : bool, optional (default=False)     If True, do not calculate or return actual aggregate. y_test : array-like, optional     Actual outcomes corresponding to the test features. Required if inference_time is False.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--returns","title":"Returns","text":"<p>dict     A dictionary with keys 'agg_predicted' and, if inference_time is False, 'agg_observed' containing the     predicted and observed respectively for the snapshot date. Each is presented as a DataFrame or an integer.</p>"},{"location":"api/#patientflow.aggregate.get_prob_dist_for_prediction_moment--raises","title":"Raises","text":"<p>ValueError     If y_test is not provided when inference_time is False.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def get_prob_dist_for_prediction_moment(\n    X_test, model, weights=None, inference_time=False, y_test=None\n):\n    \"\"\"\n    Calculate both predicted distributions and observed values for a given date using test data.\n\n    Parameters\n    ----------\n    X_test : array-like\n        Test features for a specific snapshot date.\n    model : object\n        A predictive model which should provide a `predict_proba` method.\n    weights : array-like, optional\n        Weights to apply to the predictions for aggregate calculation.\n    inference_time : bool, optional (default=False)\n        If True, do not calculate or return actual aggregate.\n    y_test : array-like, optional\n        Actual outcomes corresponding to the test features. Required if inference_time is False.\n\n    Returns\n    -------\n    dict\n        A dictionary with keys 'agg_predicted' and, if inference_time is False, 'agg_observed' containing the\n        predicted and observed respectively for the snapshot date. Each is presented as a DataFrame or an integer.\n\n    Raises\n    ------\n    ValueError\n        If y_test is not provided when inference_time is False.\n\n    \"\"\"\n    if not inference_time and y_test is None:\n        raise ValueError(\"y_test must be provided if inference_time is False.\")\n\n    prediction_moment_dict = {}\n\n    if len(X_test) &gt; 0:\n        pred_proba = model_input_to_pred_proba(X_test, model)\n        agg_predicted = pred_proba_to_agg_predicted(pred_proba, weights)\n        prediction_moment_dict[\"agg_predicted\"] = agg_predicted\n\n        if not inference_time:\n            prediction_moment_dict[\"agg_observed\"] = sum(y_test)\n    else:\n        prediction_moment_dict[\"agg_predicted\"] = pd.DataFrame(\n            {\"agg_proba\": [1]}, index=[0]\n        )\n        if not inference_time:\n            prediction_moment_dict[\"agg_observed\"] = 0\n\n    return prediction_moment_dict\n</code></pre>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba","title":"<code>model_input_to_pred_proba(model_input, model)</code>","text":"<p>Use a predictive model to convert model input data into predicted probabilities.</p>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba--parameters","title":"Parameters","text":"<p>model_input : array-like     The input data to the model, typically as features used for predictions. model : object     A model object with a <code>predict_proba</code> method that computes probability estimates.</p>"},{"location":"api/#patientflow.aggregate.model_input_to_pred_proba--returns","title":"Returns","text":"<p>DataFrame     A pandas DataFrame containing the predicted probabilities for the positive class,     with one column labeled 'pred_proba'.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def model_input_to_pred_proba(model_input, model):\n    \"\"\"\n    Use a predictive model to convert model input data into predicted probabilities.\n\n    Parameters\n    ----------\n    model_input : array-like\n        The input data to the model, typically as features used for predictions.\n    model : object\n        A model object with a `predict_proba` method that computes probability estimates.\n\n    Returns\n    -------\n    DataFrame\n        A pandas DataFrame containing the predicted probabilities for the positive class,\n        with one column labeled 'pred_proba'.\n\n    \"\"\"\n    if len(model_input) == 0:\n        return pd.DataFrame(columns=[\"pred_proba\"])\n    else:\n        predictions = model.predict_proba(model_input)[:, 1]\n        return pd.DataFrame(\n            predictions, index=model_input.index, columns=[\"pred_proba\"]\n        )\n</code></pre>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted","title":"<code>pred_proba_to_agg_predicted(predictions_proba, weights=None)</code>","text":"<p>Convert individual probability predictions into aggregate predicted probability distribution using optional weights.</p>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted--parameters","title":"Parameters","text":"<p>predictions_proba : DataFrame     A DataFrame containing the probability predictions; must have a single column named 'pred_proba'. weights : array-like, optional     An array of weights, of the same length as the DataFrame rows, to apply to each prediction.</p>"},{"location":"api/#patientflow.aggregate.pred_proba_to_agg_predicted--returns","title":"Returns","text":"<p>DataFrame     A DataFrame with a single column 'agg_proba' showing the aggregated probability,     indexed from 0 to n, where n is the number of predictions.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def pred_proba_to_agg_predicted(predictions_proba, weights=None):\n    \"\"\"\n    Convert individual probability predictions into aggregate predicted probability distribution using optional weights.\n\n    Parameters\n    ----------\n    predictions_proba : DataFrame\n        A DataFrame containing the probability predictions; must have a single column named 'pred_proba'.\n    weights : array-like, optional\n        An array of weights, of the same length as the DataFrame rows, to apply to each prediction.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame with a single column 'agg_proba' showing the aggregated probability,\n        indexed from 0 to n, where n is the number of predictions.\n\n    \"\"\"\n    n = len(predictions_proba)\n\n    if n == 0:\n        agg_predicted_dict = {0: 1}\n    else:\n        local_proba = predictions_proba.copy()\n        if weights is not None:\n            local_proba[\"pred_proba\"] *= weights\n\n        syms = create_symbols(n)\n        expression = build_expression(syms, n)\n        expression = expression_subs(expression, n, local_proba[\"pred_proba\"])\n        agg_predicted_dict = {i: return_coeff(expression, i) for i in range(n + 1)}\n\n    agg_predicted = pd.DataFrame.from_dict(\n        agg_predicted_dict, orient=\"index\", columns=[\"agg_proba\"]\n    )\n    return agg_predicted\n</code></pre>"},{"location":"api/#patientflow.aggregate.return_coeff","title":"<code>return_coeff(expression, i)</code>","text":"<p>Extract the coefficient of a specified power from an expanded symbolic expression.</p>"},{"location":"api/#patientflow.aggregate.return_coeff--parameters","title":"Parameters","text":"<p>expression : Expr     The expression to expand and extract from. i : int     The power of the term whose coefficient is to be extracted.</p>"},{"location":"api/#patientflow.aggregate.return_coeff--returns","title":"Returns","text":"<p>number     The coefficient of the specified power in the expression.</p> Source code in <code>src/patientflow/aggregate.py</code> <pre><code>def return_coeff(expression, i):\n    \"\"\"\n    Extract the coefficient of a specified power from an expanded symbolic expression.\n\n    Parameters\n    ----------\n    expression : Expr\n        The expression to expand and extract from.\n    i : int\n        The power of the term whose coefficient is to be extracted.\n\n    Returns\n    -------\n    number\n        The coefficient of the specified power in the expression.\n\n    \"\"\"\n    s = sym.Symbol(\"s\")\n    return expand(expression).coeff(s, i)\n</code></pre>"},{"location":"api/#patientflow.load","title":"<code>load</code>","text":"<p>This module provides functionality for loading configuration files, data from CSV files, and trained machine learning models.</p> <p>It includes the following features:</p> <ul> <li>Loading Configurations: Parse YAML configuration files and extract necessary parameters for data processing and modeling.</li> <li>Data Handling: Load and preprocess data from CSV files, including optional operations like setting an index, sorting, and applying literal evaluation on columns.</li> <li>Model Management: Load saved machine learning models, customize model filenames based on time, and categorize DataFrame columns into predefined groups for analysis.</li> </ul> <p>The module handles common file and parsing errors, returning appropriate error messages or exceptions.</p>"},{"location":"api/#patientflow.load--functions","title":"Functions","text":"<ul> <li><code>parse_args</code>: Parses command-line arguments for training models.</li> <li><code>load_config_file</code>: Load a YAML configuration file and extract key parameters.</li> <li><code>set_data_file_names</code>: Set file locations based on UCLH-specific or default data sources.</li> <li><code>safe_literal_eval</code>: Safely evaluate string literals into Python objects.</li> <li><code>data_from_csv</code>: Load and preprocess data from a CSV file.</li> <li><code>get_model_name</code>: Generate a model name based on the time of day.</li> <li><code>load_saved_model</code>: Load a machine learning model saved in a joblib file.</li> <li><code>get_dict_cols</code>: Categorize columns from a DataFrame into predefined groups for analysis.</li> </ul>"},{"location":"api/#patientflow.load.data_from_csv","title":"<code>data_from_csv(csv_path, index_column=None, sort_columns=None, eval_columns=None)</code>","text":"<p>Loads data from csv file</p> <p>Args: csv_path (str): The path to the ED data file index_column (str): The column to set as index sort_columns (list): The columns to sort the dataframe by eval_columns (list): The columns to apply safe_literal_eval to</p> <p>Returns: pd.DataFrame: A dataframe with the ED visits. See data dictionary</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def data_from_csv(csv_path, index_column=None, sort_columns=None, eval_columns=None):\n    \"\"\"\n    Loads data from csv file\n\n    Args:\n    csv_path (str): The path to the ED data file\n    index_column (str): The column to set as index\n    sort_columns (list): The columns to sort the dataframe by\n    eval_columns (list): The columns to apply safe_literal_eval to\n\n    Returns:\n    pd.DataFrame: A dataframe with the ED visits. See data dictionary\n\n    \"\"\"\n    path = os.path.join(Path().home(), csv_path)\n\n    if not os.path.exists(path):\n        print(f\"Data file not found at path: {path}\")\n        sys.exit(1)\n\n    try:\n        df = pd.read_csv(path, parse_dates=True)\n    except FileNotFoundError:\n        print(f\"Data file not found at path: {path}\")\n        sys.exit(1)\n    except Exception as e:\n        print(f\"Error loading data: {e}\")\n        sys.exit(1)\n\n    if index_column:\n        try:\n            if df.index.name != index_column:\n                df = df.set_index(index_column)\n        except KeyError:\n            print(f\"Index column '{index_column}' not found in dataframe\")\n\n    if sort_columns:\n        try:\n            df.sort_values(sort_columns, inplace=True)\n        except KeyError:\n            print(\"One or more sort columns not found in dataframe\")\n\n    if eval_columns:\n        for column in eval_columns:\n            if column in df.columns:\n                try:\n                    df[column] = df[column].apply(safe_literal_eval)\n                except Exception as e:\n                    print(f\"Error applying safe_literal_eval to column '{column}': {e}\")\n\n    return df\n</code></pre>"},{"location":"api/#patientflow.load.get_dict_cols","title":"<code>get_dict_cols(df)</code>","text":"<p>Categorize DataFrame columns into predefined groups.</p>"},{"location":"api/#patientflow.load.get_dict_cols--parameters","title":"Parameters","text":"<p>df : pd.DataFrame     The DataFrame to categorize.</p>"},{"location":"api/#patientflow.load.get_dict_cols--returns","title":"Returns","text":"<p>dict     A dictionary where keys are column group names and values are lists of column names in each group.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def get_dict_cols(df):\n    \"\"\"\n    Categorize DataFrame columns into predefined groups.\n\n    Parameters\n    ----------\n    df : pd.DataFrame\n        The DataFrame to categorize.\n\n    Returns\n    -------\n    dict\n        A dictionary where keys are column group names and values are lists of column names in each group.\n    \"\"\"\n    not_used_in_training_vars = [\n        \"snapshot_id\",\n        \"snapshot_date\",\n        \"prediction_time\",\n        \"visit_number\",\n        \"training_validation_test\",\n        \"random_number\",\n    ]\n    arrival_and_demographic_vars = [\n        \"elapsed_los\",\n        \"sex\",\n        \"age_group\",\n        \"age_on_arrival\",\n        \"arrival_method\",\n    ]\n    summary_vars = [\n        \"num_obs\",\n        \"num_obs_events\",\n        \"num_obs_types\",\n        \"num_lab_batteries_ordered\",\n    ]\n\n    location_vars = []\n    observations_vars = []\n    labs_vars = []\n    consults_vars = [\n        \"has_consultation\",\n        \"consultation_sequence\",\n        \"final_sequence\",\n        \"specialty\",\n    ]\n    outcome_vars = [\"is_admitted\"]\n\n    for col in df.columns:\n        if (\n            col in not_used_in_training_vars\n            or col in arrival_and_demographic_vars\n            or col in summary_vars\n        ):\n            continue\n        elif \"visited\" in col or \"location\" in col:\n            location_vars.append(col)\n        elif \"num_obs\" in col or \"latest_obs\" in col:\n            observations_vars.append(col)\n        elif \"lab_orders\" in col or \"latest_lab_results\" in col:\n            labs_vars.append(col)\n        elif col in consults_vars or col in outcome_vars:\n            continue  # Already categorized\n        else:\n            print(f\"Column '{col}' did not match any predefined group\")\n\n    # Create a list of column groups\n    col_group_names = [\n        \"not used in training\",\n        \"arrival and demographic\",\n        \"summary\",\n        \"location\",\n        \"observations\",\n        \"lab orders and results\",\n        \"consults\",\n        \"outcome\",\n    ]\n\n    # Create a list of the column names within those groups\n    col_groups = [\n        not_used_in_training_vars,\n        arrival_and_demographic_vars,\n        summary_vars,\n        location_vars,\n        observations_vars,\n        labs_vars,\n        consults_vars,\n        outcome_vars,\n    ]\n\n    # Use dictionary to combine them\n    dict_col_groups = {\n        category: var_list for category, var_list in zip(col_group_names, col_groups)\n    }\n\n    return dict_col_groups\n</code></pre>"},{"location":"api/#patientflow.load.get_model_name","title":"<code>get_model_name(model_name, prediction_time)</code>","text":"<p>Create a model name based on the time of day.</p>"},{"location":"api/#patientflow.load.get_model_name--parameters","title":"Parameters","text":"<p>model_name : str     The base name of the model. prediction_time_ : tuple of int     A tuple representing the time of day (hour, minute).</p>"},{"location":"api/#patientflow.load.get_model_name--returns","title":"Returns","text":"<p>str     A string representing the model name based on the time of day.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def get_model_name(model_name, prediction_time):\n    \"\"\"\n    Create a model name based on the time of day.\n\n    Parameters\n    ----------\n    model_name : str\n        The base name of the model.\n    prediction_time_ : tuple of int\n        A tuple representing the time of day (hour, minute).\n\n    Returns\n    -------\n    str\n        A string representing the model name based on the time of day.\n    \"\"\"\n\n    hour_, min_ = prediction_time\n    min_ = f\"{min_}0\" if min_ % 60 == 0 else str(min_)\n    model_name = model_name + \"_\" + f\"{hour_:02}\" + min_\n    return model_name\n</code></pre>"},{"location":"api/#patientflow.load.load_config_file","title":"<code>load_config_file(config_file_path, return_start_end_dates=False)</code>","text":"<p>Load configuration from a YAML file.</p>"},{"location":"api/#patientflow.load.load_config_file--parameters","title":"Parameters","text":"<p>config_file_path : str     The path to the configuration file. return_start_end_dates : bool, optional     If True, return only the start and end dates from the file (default is False).</p>"},{"location":"api/#patientflow.load.load_config_file--returns","title":"Returns","text":"<p>dict or tuple or None     If <code>return_start_end_dates</code> is True, returns a tuple of start and end dates (str).     Otherwise, returns a dictionary containing the configuration parameters.     Returns None if an error occurs during file reading or parsing.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def load_config_file(\n    config_file_path: str, return_start_end_dates: bool = False\n) -&gt; Optional[Union[Dict[str, Any], Tuple[str, str]]]:\n    \"\"\"\n    Load configuration from a YAML file.\n\n    Parameters\n    ----------\n    config_file_path : str\n        The path to the configuration file.\n    return_start_end_dates : bool, optional\n        If True, return only the start and end dates from the file (default is False).\n\n    Returns\n    -------\n    dict or tuple or None\n        If `return_start_end_dates` is True, returns a tuple of start and end dates (str).\n        Otherwise, returns a dictionary containing the configuration parameters.\n        Returns None if an error occurs during file reading or parsing.\n    \"\"\"\n    try:\n        with open(config_file_path, \"r\") as file:\n            config = yaml.safe_load(file)\n    except FileNotFoundError:\n        print(f\"Error: The file '{config_file_path}' was not found.\")\n        return None\n    except yaml.YAMLError as e:\n        print(f\"Error parsing YAML file: {e}\")\n        return None\n\n    try:\n        if return_start_end_dates:\n            # load the dates used in saved data for uclh versions\n            if \"file_dates\" in config and config[\"file_dates\"]:\n                start_date, end_date = [str(item) for item in config[\"file_dates\"]]\n                return (start_date, end_date)\n            else:\n                print(\n                    \"Error: 'file_dates' key not found or empty in the configuration file.\"\n                )\n                return None\n\n        params: Dict[str, Any] = {}\n\n        if \"prediction_times\" in config:\n            params[\"prediction_times\"] = [\n                tuple(item) for item in config[\"prediction_times\"]\n            ]\n        else:\n            print(\"Error: 'prediction_times' key not found in the configuration file.\")\n            sys.exit(1)\n\n        if \"modelling_dates\" in config and len(config[\"modelling_dates\"]) == 4:\n            (\n                params[\"start_training_set\"],\n                params[\"start_validation_set\"],\n                params[\"start_test_set\"],\n                params[\"end_test_set\"],\n            ) = [item for item in config[\"modelling_dates\"]]\n        else:\n            print(\n                f\"Error: expecting 4 modelling dates and only got {len(config.get('modelling_dates', []))}\"\n            )\n            return None\n\n        params[\"x1\"] = float(config.get(\"x1\", 4))\n        params[\"y1\"] = float(config.get(\"y1\", 0.76))\n        params[\"x2\"] = float(config.get(\"x2\", 12))\n        params[\"y2\"] = float(config.get(\"y2\", 0.99))\n        params[\"prediction_window\"] = config.get(\"prediction_window\", 480)\n        params[\"epsilon\"] = config.get(\"epsilon\", 10**-7)\n        params[\"yta_time_interval\"] = config.get(\"yta_time_interval\", 15)\n\n        return params\n\n    except KeyError as e:\n        print(f\"Error: Missing key in the configuration file: {e}\")\n        return None\n    except ValueError as e:\n        print(f\"Error: Invalid value found in the configuration file: {e}\")\n        return None\n</code></pre>"},{"location":"api/#patientflow.load.load_saved_model","title":"<code>load_saved_model(model_file_path, model_name, prediction_time=None)</code>","text":"<p>Load a saved model from a file.</p>"},{"location":"api/#patientflow.load.load_saved_model--parameters","title":"Parameters","text":"<p>model_file_path : Path     The path to the directory where the model is saved. model_name : str     The base name of the model. prediction_time : tuple of int, optional     The time of day the model was trained for.</p>"},{"location":"api/#patientflow.load.load_saved_model--returns","title":"Returns","text":"<p>Any     The loaded model.</p>"},{"location":"api/#patientflow.load.load_saved_model--raises","title":"Raises","text":"<p>ModelLoadError     If the model file cannot be found or loaded.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def load_saved_model(model_file_path, model_name, prediction_time=None):\n    \"\"\"\n    Load a saved model from a file.\n\n    Parameters\n    ----------\n    model_file_path : Path\n        The path to the directory where the model is saved.\n    model_name : str\n        The base name of the model.\n    prediction_time : tuple of int, optional\n        The time of day the model was trained for.\n\n    Returns\n    -------\n    Any\n        The loaded model.\n\n    Raises\n    ------\n    ModelLoadError\n        If the model file cannot be found or loaded.\n    \"\"\"\n    if prediction_time:\n        # retrieve model based on the time of day it is trained for\n        model_name = get_model_name(model_name, prediction_time)\n\n    full_path = model_file_path / model_name\n    full_path = full_path.with_suffix(\".joblib\")\n\n    try:\n        model = load(full_path)\n        return model\n    except FileNotFoundError:\n        # print(f\"Model named {model_name} not found at path: {model_file_path}\")\n        raise ModelLoadError(\n            f\"Model named {model_name} not found at path: {model_file_path}\"\n        )\n    except Exception as e:\n        # print(f\"Error loading model: {e}\")\n        raise ModelLoadError(f\"Error loading model called {model_name}: {e}\")\n</code></pre>"},{"location":"api/#patientflow.load.parse_args","title":"<code>parse_args()</code>","text":"<p>Parse command-line arguments for the training script.</p> <p>Returns:</p> Type Description <code>Namespace</code> <p>argparse.Namespace: The parsed arguments containing 'data_folder_name' and 'uclh' keys.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def parse_args() -&gt; argparse.Namespace:\n    \"\"\"\n    Parse command-line arguments for the training script.\n\n    Returns:\n        argparse.Namespace: The parsed arguments containing 'data_folder_name' and 'uclh' keys.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Train emergency demand models\")\n    parser.add_argument(\n        \"--data_folder_name\",\n        type=str,\n        default=\"data-synthetic\",\n        help=\"Location of data for training\",\n    )\n    parser.add_argument(\n        \"--uclh\",\n        type=lambda x: x.lower() in [\"true\", \"1\", \"yes\", \"y\"],\n        default=False,\n        help=\"Train using UCLH data (True) or Public data (False)\",\n    )\n    args = parser.parse_args()\n    return args\n</code></pre>"},{"location":"api/#patientflow.load.safe_literal_eval","title":"<code>safe_literal_eval(s)</code>","text":"<p>Safely evaluate a string literal into a Python object. Handles list-like strings by converting them to lists.</p>"},{"location":"api/#patientflow.load.safe_literal_eval--parameters","title":"Parameters","text":"<p>s : str     The string to evaluate.</p>"},{"location":"api/#patientflow.load.safe_literal_eval--returns","title":"Returns","text":"<p>Any, list, or None     The evaluated Python object if successful, a list if the input is list-like,     or None for empty/null values.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def safe_literal_eval(s):\n    \"\"\"\n    Safely evaluate a string literal into a Python object.\n    Handles list-like strings by converting them to lists.\n\n    Parameters\n    ----------\n    s : str\n        The string to evaluate.\n\n    Returns\n    -------\n    Any, list, or None\n        The evaluated Python object if successful, a list if the input is list-like,\n        or None for empty/null values.\n    \"\"\"\n    if pd.isna(s) or str(s).strip().lower() in [\"nan\", \"none\", \"\"]:\n        return None\n\n    if isinstance(s, str):\n        s = s.strip()\n        if s.startswith(\"[\") and s.endswith(\"]\"):\n            try:\n                # Remove square brackets and split by comma\n                items = s[1:-1].split(\",\")\n                # Strip whitespace from each item and remove empty strings\n                return [item.strip() for item in items if item.strip()]\n            except Exception:\n                # If the above fails, fall back to ast.literal_eval\n                pass\n\n    try:\n        return ast.literal_eval(s)\n    except (ValueError, SyntaxError):\n        # If ast.literal_eval fails, return the original string\n        return s\n</code></pre>"},{"location":"api/#patientflow.load.set_data_file_names","title":"<code>set_data_file_names(uclh, data_file_path, config_file_path=None)</code>","text":"<p>Set file locations based on UCLH or default data source.</p>"},{"location":"api/#patientflow.load.set_data_file_names--parameters","title":"Parameters","text":"<p>uclh : bool     If True, use UCLH-specific file locations. If False, use default file locations. data_file_path : Path     The base path to the data directory. config_file_path : str, optional     The path to the configuration file, required if <code>uclh</code> is True.</p>"},{"location":"api/#patientflow.load.set_data_file_names--returns","title":"Returns","text":"<p>tuple     Paths to the required files (visits, arrivals) based on the configuration.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def set_data_file_names(uclh, data_file_path, config_file_path=None):\n    \"\"\"\n    Set file locations based on UCLH or default data source.\n\n    Parameters\n    ----------\n    uclh : bool\n        If True, use UCLH-specific file locations. If False, use default file locations.\n    data_file_path : Path\n        The base path to the data directory.\n    config_file_path : str, optional\n        The path to the configuration file, required if `uclh` is True.\n\n    Returns\n    -------\n    tuple\n        Paths to the required files (visits, arrivals) based on the configuration.\n    \"\"\"\n    if not isinstance(data_file_path, Path):\n        data_file_path = Path(data_file_path)\n\n    if not uclh:\n        csv_filename = \"ed_visits.csv\"\n        yta_csv_filename = \"inpatient_arrivals.csv\"\n\n        visits_csv_path = data_file_path / csv_filename\n        yta_csv_path = data_file_path / yta_csv_filename\n\n        return visits_csv_path, yta_csv_path\n\n    else:\n        start_date, end_date = load_config_file(\n            config_file_path, return_start_end_dates=True\n        )\n        data_filename = (\n            \"uclh_visits_exc_beds_inc_minority_\"\n            + str(start_date)\n            + \"_\"\n            + str(end_date)\n            + \".pickle\"\n        )\n        csv_filename = \"uclh_ed_visits.csv\"\n        yta_filename = (\n            \"uclh_yet_to_arrive_\" + str(start_date) + \"_\" + str(end_date) + \".pickle\"\n        )\n        yta_csv_filename = \"uclh_inpatient_arrivals.csv\"\n\n        visits_path = data_file_path / data_filename\n        yta_path = data_file_path / yta_filename\n\n        visits_csv_path = data_file_path / csv_filename\n        yta_csv_path = data_file_path / yta_csv_filename\n\n    return visits_path, visits_csv_path, yta_path, yta_csv_path\n</code></pre>"},{"location":"api/#patientflow.load.set_file_paths","title":"<code>set_file_paths(inference_time, train_dttm, data_folder_name, uclh=False, from_notebook=False, prefix='admissions')</code>","text":"<p>Sets up the file paths and loads configuration parameters from a YAML file.</p> <p>Parameters:</p> Name Type Description Default <code>inference_time</code> <code>bool</code> <p>A flag indicating whether it is inference time or not</p> required <code>train_dttm</code> <code>str</code> <p>A string representation of the datetime at which training commenced</p> required <code>data_folder_name</code> <code>str</code> <p>Name of the folder where data files are located.</p> required <code>uclh</code> <code>bool</code> <p>A flag indicating whether to use UCLH-specific configuration files and data paths.          Default is False.</p> <code>False</code> <code>prefix</code> <code>str</code> <p>A string to include at the beginning of the folder name in which the models will be saved</p> <code>'admissions'</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[Path, Path, Path, Path]</code> <p>A tuple containing the following elements: - data_file_path (Path): Path to the data folder. - media_file_path (Path): Path to the media folder (created if not already existing). - model_file_path (Path): Path to the trained models folder (created if not already existing). - config_path (Path): Path to the configuration file used.</p> Source code in <code>src/patientflow/load.py</code> <pre><code>def set_file_paths(\n    inference_time: bool,\n    train_dttm: str,\n    data_folder_name: str,\n    uclh: bool = False,\n    from_notebook: bool = False,\n    prefix: str = \"admissions\",\n) -&gt; Tuple[Path, Path, Path, Path]:\n    \"\"\"\n    Sets up the file paths and loads configuration parameters from a YAML file.\n\n    Args:\n        inference_time (bool): A flag indicating whether it is inference time or not\n        train_dttm (str): A string representation of the datetime at which training commenced\n        data_folder_name (str): Name of the folder where data files are located.\n        uclh (bool): A flag indicating whether to use UCLH-specific configuration files and data paths.\n                     Default is False.\n        prefix: A string to include at the beginning of the folder name in which the models will be saved\n\n    Returns:\n        tuple: A tuple containing the following elements:\n            - data_file_path (Path): Path to the data folder.\n            - media_file_path (Path): Path to the media folder (created if not already existing).\n            - model_file_path (Path): Path to the trained models folder (created if not already existing).\n            - config_path (Path): Path to the configuration file used.\n    \"\"\"\n    # Get the current path and root\n    if from_notebook:\n        root = Path().resolve().parent\n    else:\n        current_path = Path(__file__)\n        root = current_path.parents[2]\n\n    # Set config file based on the `uclh` flag\n    if uclh:\n        config_path = Path(root) / \"config-uclh.yaml\"\n    else:\n        config_path = Path(root) / \"config.yaml\"\n    print(f\"Configuration will be loaded from: {config_path}\")\n\n    # Set data and media file paths\n    data_file_path = Path(root) / data_folder_name\n    print(f\"Data files will be loaded from: {data_file_path}\")\n\n    # Create model ID from current date, data_folder_name\n    model_id = prefix + \"_\" + data_folder_name.lstrip(\"data-\")\n\n    if inference_time:\n        if uclh:\n            # at inference time, if uclh, require a train_dttm in order to identify the correct model\n            if train_dttm:\n                model_id = model_id + \"_\" + train_dttm\n                model_file_path = Path(root) / \"trained-models\" / model_id\n            else:\n                raise ModelLoadError(\n                    \"Please specify train_dttm of required model so that it can be loaded\"\n                )\n        else:\n            # use a train_dttm if provided; if not use any model\n            if train_dttm:\n                model_id = model_id + \"_\" + train_dttm\n            if from_notebook:\n                model_file_path = Path(root) / \"trained-models\"\n            else:\n                model_file_path = Path(root) / \"trained-models\" / model_id\n        if from_notebook:\n            media_file_path = Path(root) / \"notebooks\" / \"img\"\n        else:\n            media_file_path = model_file_path / \"media\"\n\n    else:  # not inference time\n        if train_dttm:\n            model_id = model_id + \"_\" + train_dttm\n\n        if from_notebook:\n            model_file_path = Path(root) / \"trained-models\"\n        else:\n            model_file_path = Path(root) / \"trained-models\" / model_id\n\n        print(f\"Trained models will be saved to: {model_file_path}\")\n        model_file_path.mkdir(parents=True, exist_ok=True)\n\n        filename_results_dict_path = model_file_path / \"model-output\"\n        filename_results_dict_path.mkdir(parents=False, exist_ok=True)\n\n        if from_notebook:\n            media_file_path = Path(root) / \"notebooks\" / \"img\"\n        else:\n            media_file_path = model_file_path / \"media\"\n        media_file_path.mkdir(parents=False, exist_ok=True)\n        print(f\"Images will be saved to: {media_file_path}\")\n\n    # Return paths and parameters\n    return data_file_path, media_file_path, model_file_path, config_path\n</code></pre>"},{"location":"api/#patientflow.predict","title":"<code>predict</code>","text":""},{"location":"api/#patientflow.predict.admission_in_prediction_window","title":"<code>admission_in_prediction_window</code>","text":"<p>This module provides functions to model and analyze a curve consisting of an exponential growth segment followed by an exponential decay segment. It includes functions to create the curve, calculate specific points on it, and evaluate probabilities based on its shape.</p> <p>Its intended use is to derive the probability of a patient being admitted to a hospital within a certain elapsed time after their arrival in the Emergency Department (ED), given the hospital's aspirations for the time it takes patients to be admitted. For this purpose, two points on the curve are required as parameters: (x1,y1) : The target proportion of patients y1 (eg 76%) who have been admitted or discharged by time x1 (eg 4 hours). It is assumed that values of y where x &lt; x1 is a growth curve grow exponentially towards x1 and that (x1,y1) the curve switches to a decay curve (x2, y2) : The time x2 by which all but a small proportion y2 of patients have been admitted.</p> <p>The curve is generated by the following functions:</p> <ul> <li>growth_curve: Calculate exponential growth at a point where x &lt; x1.</li> <li>decay_curve: Calculate exponential decay at a point where x &gt;= x1.</li> <li>create_curve: Generate a full curve with both growth and decay segments.</li> </ul> <p>The curve is applied as follows - get_y_from_aspirational_curve: Read from the curve a value for y, the probability of being admitted, for a given moment x hours after arrival - calculate_probability: Compute the probability of a patient being admitted by the end of a prediction window, given how much time has elapsed since their arrival.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.calculate_probability","title":"<code>calculate_probability(elapsed_los_td_hrs, prediction_window_hrs, x1, y1, x2, y2)</code>","text":"<p>Calculates the probability of an admission occurring within a specified prediction window after the moment of prediction, based on the patient's elapsed time in the ED prior to the moment of prediction and the length of the window</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.calculate_probability--parameters","title":"Parameters","text":"<p>elapsed_los_td_hrs : float     The elapsed time since the patient arrived at the ED. prediction_window_hrs : float     The duration of the prediction window after the point of prediction, for which the probability is calculated. (x1,y1) :     An aspirational target, expressed in the form of a point on a curve, representing a proportion of patients y1 (eg 76%) who - if the ED meets its targets - will be admitted or discharged by time x1 (eg 4 hours). (x2,y2) :     An aspirational target, expressed in the form of a point on a curve, representing a time x2 by which all but a small proportion y2 of patients have been admitted if the ED meets its targets.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.calculate_probability--returns","title":"Returns","text":"<p>float     The probability of the event occurring within the given prediction window.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.calculate_probability--edge-case-handling","title":"Edge Case Handling","text":"<p>When elapsed_los_td_hrs is extremely high, such as values significantly greater than x2, the admission probability prior to the current time (<code>prob_admission_prior_to_now</code>) can reach 1.0 despite the curve being asymptotic. This scenario can cause computational errors when calculating the conditional probability, as it involves a division by zero. In such cases, this function directly returns a probability of 1.0, reflecting certainty of admission.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.calculate_probability--example","title":"Example","text":"<p>Calculate the probability that a patient, who has already been in the ED for 3 hours, will be admitted in the next 2 hours. The ED targets that 76% of patients are admitted or discharged within 4 hours, and 99% within 12 hours.</p> <p>calculate_probability(3, 2, 4, 0.76, 12, 0.99)</p> Source code in <code>src/patientflow/predict/admission_in_prediction_window.py</code> <pre><code>def calculate_probability(elapsed_los_td_hrs, prediction_window_hrs, x1, y1, x2, y2):\n    \"\"\"\n    Calculates the probability of an admission occurring within a specified prediction window after the moment of prediction, based on the patient's elapsed time in the ED prior to the moment of prediction and the length of the window\n\n    Parameters\n    ----------\n    elapsed_los_td_hrs : float\n        The elapsed time since the patient arrived at the ED.\n    prediction_window_hrs : float\n        The duration of the prediction window after the point of prediction, for which the probability is calculated.\n    (x1,y1) :\n        An aspirational target, expressed in the form of a point on a curve, representing a proportion of patients y1 (eg 76%) who - if the ED meets its targets - will be admitted or discharged by time x1 (eg 4 hours).\n    (x2,y2) :\n        An aspirational target, expressed in the form of a point on a curve, representing a time x2 by which all but a small proportion y2 of patients have been admitted if the ED meets its targets.\n\n    Returns\n    -------\n    float\n        The probability of the event occurring within the given prediction window.\n\n    Edge Case Handling\n    ------------------\n    When elapsed_los_td_hrs is extremely high, such as values significantly greater than x2, the admission probability prior to the current time (`prob_admission_prior_to_now`) can reach 1.0 despite the curve being asymptotic. This scenario can cause computational errors when calculating the conditional probability, as it involves a division by zero. In such cases, this function directly returns a probability of 1.0, reflecting certainty of admission.\n\n    Example\n    -------\n    Calculate the probability that a patient, who has already been in the ED for 3 hours, will be admitted in the next 2 hours. The ED targets that 76% of patients are admitted or discharged within 4 hours, and 99% within 12 hours.\n\n    &gt;&gt;&gt; calculate_probability(3, 2, 4, 0.76, 12, 0.99)\n\n    \"\"\"\n    # probability of still being in the ED now (a function of elapsed time since arrival)\n    prob_admission_prior_to_now = get_y_from_aspirational_curve(\n        elapsed_los_td_hrs, x1, y1, x2, y2\n    )\n\n    # prob admission when adding the prediction window added to elapsed time since arrival\n    prob_admission_by_end_of_window = get_y_from_aspirational_curve(\n        elapsed_los_td_hrs + prediction_window_hrs, x1, y1, x2, y2\n    )\n\n    # Direct return for edge cases where `prob_admission_prior_to_now` reaches 1.0\n    if prob_admission_prior_to_now == 1:\n        return 1.0\n\n    # Calculate conditional probability within the prediction window\n    return (prob_admission_by_end_of_window - prob_admission_prior_to_now) / (\n        1 - prob_admission_prior_to_now\n    )\n</code></pre>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.create_curve","title":"<code>create_curve(x1, y1, x2, y2, a=0.01, generate_values=False)</code>","text":"<p>Generates parameters for an exponential growth and decay curve. Optionally generates x-values and corresponding y-values across a default or specified range.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.create_curve--parameters","title":"Parameters","text":"<p>x1 : float     The x-value where the curve transitions from growth to decay. y1 : float     The y-value at the transition point x1. x2 : float     The x-value defining the end of the decay curve for calculation purposes. y2 : float     The y-value at x2, intended to fine-tune the decay rate. a : float, optional     The initial value coefficient for the growth curve, defaults to 0.01. generate_values : bool, optional     Flag to determine whether to generate x-values and y-values for visualization purposes.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.create_curve--returns","title":"Returns","text":"<p>tuple     If generate_values is False, returns (gamma, lamda, a).     If generate_values is True, returns (gamma, lamda, a, x_values, y_values).</p> Source code in <code>src/patientflow/predict/admission_in_prediction_window.py</code> <pre><code>def create_curve(x1, y1, x2, y2, a=0.01, generate_values=False):\n    \"\"\"\n    Generates parameters for an exponential growth and decay curve.\n    Optionally generates x-values and corresponding y-values across a default or specified range.\n\n    Parameters\n    ----------\n    x1 : float\n        The x-value where the curve transitions from growth to decay.\n    y1 : float\n        The y-value at the transition point x1.\n    x2 : float\n        The x-value defining the end of the decay curve for calculation purposes.\n    y2 : float\n        The y-value at x2, intended to fine-tune the decay rate.\n    a : float, optional\n        The initial value coefficient for the growth curve, defaults to 0.01.\n    generate_values : bool, optional\n        Flag to determine whether to generate x-values and y-values for visualization purposes.\n\n    Returns\n    -------\n    tuple\n        If generate_values is False, returns (gamma, lamda, a).\n        If generate_values is True, returns (gamma, lamda, a, x_values, y_values).\n\n    \"\"\"\n    # Validate inputs\n    if not (x1 &lt; x2):\n        raise ValueError(\"x1 must be less than x2\")\n    if not (0 &lt; y1 &lt; y2 &lt; 1):\n        raise ValueError(\"y1 must be less than y2, and both must be between 0 and 1\")\n\n    # Constants for growth and decay\n    gamma = np.log(y1 / a) / x1\n    lamda = np.log((1 - y1) / (1 - y2)) / (x2 - x1)\n\n    if generate_values:\n        x_values = np.linspace(0, 20, 200)\n        y_values = [\n            (growth_curve(x, a, gamma) if x &lt;= x1 else decay_curve(x, x1, y1, lamda))\n            for x in x_values\n        ]\n        return gamma, lamda, a, x_values, y_values\n\n    return gamma, lamda, a\n</code></pre>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.decay_curve","title":"<code>decay_curve(x, x1, y1, lamda)</code>","text":"<p>Calculate the exponential decay value at a given x using specified parameters. The function supports both scalar and array inputs for x.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.decay_curve--parameters","title":"Parameters","text":"<p>x : float or np.ndarray     The x-value(s) at which to evaluate the curve. x1 : float     The x-value where the growth curve transitions to the decay curve. y1 : float     The y-value at the transition point, where the decay curve starts. lamda : float     The decay rate coefficient.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.decay_curve--returns","title":"Returns","text":"<p>float or np.ndarray     The y-value(s) of the decay curve at x.</p> Source code in <code>src/patientflow/predict/admission_in_prediction_window.py</code> <pre><code>def decay_curve(x, x1, y1, lamda):\n    \"\"\"\n    Calculate the exponential decay value at a given x using specified parameters.\n    The function supports both scalar and array inputs for x.\n\n    Parameters\n    ----------\n    x : float or np.ndarray\n        The x-value(s) at which to evaluate the curve.\n    x1 : float\n        The x-value where the growth curve transitions to the decay curve.\n    y1 : float\n        The y-value at the transition point, where the decay curve starts.\n    lamda : float\n        The decay rate coefficient.\n\n    Returns\n    -------\n    float or np.ndarray\n        The y-value(s) of the decay curve at x.\n\n    \"\"\"\n    return y1 + (1 - y1) * (1 - np.exp(-lamda * (x - x1)))\n</code></pre>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.get_y_from_aspirational_curve","title":"<code>get_y_from_aspirational_curve(x, x1, y1, x2, y2)</code>","text":"<p>Calculate the probability y that a patient will have been admitted by a specified x after their arrival, by reading from the aspirational curve that has been constrained to pass through points (x1, y1) and (x2, y2) with an exponential growth curve where x &lt; x1 and an exponential decay where x &lt; x2</p> <p>The function handles scalar or array inputs for x and determines y using either an exponential growth curve (for x &lt; x1) or an exponential decay curve (for x &gt;= x1). The curve parameters are derived to ensure the curve passes through specified points (x1, y1) and (x2, y2).</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.get_y_from_aspirational_curve--parameters","title":"Parameters","text":"<p>x : float or np.ndarray     The x-coordinate(s) at which to calculate the y-value on the curve. Can be a single value or an array of values. x1 : float     The x-coordinate of the first key point on the curve, where the growth phase ends and the decay phase begins. y1 : float     The y-coordinate of the first key point (x1), representing the target proportion of patients admitted by time x1. x2 : float     The x-coordinate of the second key point on the curve, beyond which all but a few patients are expected to be admitted. y2 : float     The y-coordinate of the second key point (x2), representing the target proportion of patients admitted by time x2.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.get_y_from_aspirational_curve--returns","title":"Returns","text":"<p>float or np.ndarray     The calculated y-value(s) (probability of admission) at the given x. The type of the return matches the input type     for x (either scalar or array).</p> Source code in <code>src/patientflow/predict/admission_in_prediction_window.py</code> <pre><code>def get_y_from_aspirational_curve(x, x1, y1, x2, y2):\n    \"\"\"\n    Calculate the probability y that a patient will have been admitted by a specified x after their arrival, by reading from the aspirational curve that has been constrained to pass through points (x1, y1) and (x2, y2) with an exponential growth curve where x &lt; x1 and an exponential decay where x &lt; x2\n\n    The function handles scalar or array inputs for x and determines y using either an exponential growth curve (for x &lt; x1)\n    or an exponential decay curve (for x &gt;= x1). The curve parameters are derived to ensure the curve passes through\n    specified points (x1, y1) and (x2, y2).\n\n    Parameters\n    ----------\n    x : float or np.ndarray\n        The x-coordinate(s) at which to calculate the y-value on the curve. Can be a single value or an array of values.\n    x1 : float\n        The x-coordinate of the first key point on the curve, where the growth phase ends and the decay phase begins.\n    y1 : float\n        The y-coordinate of the first key point (x1), representing the target proportion of patients admitted by time x1.\n    x2 : float\n        The x-coordinate of the second key point on the curve, beyond which all but a few patients are expected to be admitted.\n    y2 : float\n        The y-coordinate of the second key point (x2), representing the target proportion of patients admitted by time x2.\n\n    Returns\n    -------\n    float or np.ndarray\n        The calculated y-value(s) (probability of admission) at the given x. The type of the return matches the input type\n        for x (either scalar or array).\n\n    \"\"\"\n    gamma, lamda, a = create_curve(x1, y1, x2, y2)\n    y = np.where(x &lt; x1, growth_curve(x, a, gamma), decay_curve(x, x1, y1, lamda))\n    return y\n</code></pre>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.growth_curve","title":"<code>growth_curve(x, a, gamma)</code>","text":"<p>Calculate the exponential growth value at a given x using specified parameters. The function supports both scalar and array inputs for x.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.growth_curve--parameters","title":"Parameters","text":"<p>x : float or np.ndarray     The x-value(s) at which to evaluate the curve. a : float     The coefficient that defines the starting point of the growth curve when x is 0. gamma : float     The growth rate coefficient of the curve.</p>"},{"location":"api/#patientflow.predict.admission_in_prediction_window.growth_curve--returns","title":"Returns","text":"<p>float or np.ndarray     The y-value(s) of the growth curve at x.</p> Source code in <code>src/patientflow/predict/admission_in_prediction_window.py</code> <pre><code>def growth_curve(x, a, gamma):\n    \"\"\"\n    Calculate the exponential growth value at a given x using specified parameters.\n    The function supports both scalar and array inputs for x.\n\n    Parameters\n    ----------\n    x : float or np.ndarray\n        The x-value(s) at which to evaluate the curve.\n    a : float\n        The coefficient that defines the starting point of the growth curve when x is 0.\n    gamma : float\n        The growth rate coefficient of the curve.\n\n    Returns\n    -------\n    float or np.ndarray\n        The y-value(s) of the growth curve at x.\n\n    \"\"\"\n    return a * np.exp(x * gamma)\n</code></pre>"},{"location":"api/#patientflow.predict.realtime_demand","title":"<code>realtime_demand</code>","text":""},{"location":"api/#patientflow.predict.realtime_demand.create_predictions","title":"<code>create_predictions(model_file_path, prediction_time, prediction_snapshots, specialties, prediction_window_hrs, x1, y1, x2, y2, cdf_cut_points, special_params=None)</code>","text":"<p>Create predictions for emergency demand for a single prediction moment.</p> <p>Parameters: - model_file_path (str): Path to the model files. - prediction_moment (Tuple): Hour and minute of time for which model to be used for inference was trained - prediction_snapshots (pd.DataFrame): DataFrame containing prediction snapshots. - specialties (List[str]): List of specialty names for which predictions are required. - prediction_window_hrs (float): Prediction window in hours. - x1, y1, x2, y2 (float): Parameters for calculating probability of admission within prediction window. - cdf_cut_points (List[float]): List of cumulative distribution function cut points. - special_params (Optional[Dict[str, Any]]): Dictionary containing 'special_category_func', 'special_category_dict', and 'special_func_map'.   - special_category_func (Callable[[Any], Any]): Function identifying patients whose specialty predictions are handled outside the get_specialty_probs() function.   - special_category_dict (Dict[str, Any]): Dictionary of probabilities applied to those patients.   - special_func_map (Dict[str, Callable[[pd.Series], bool]]): A dictionary mapping specialties to specific functions that are applied to each row of the prediction snapshots to filter indices.</p> <p>Returns: - Dict[str, Dict[str, List[int]]]: Predictions for each specialty.</p> <p>Example:</p> <pre><code>from datetime import datetime\nimport pandas as pd\n\nspecial_category_dict = {\n    'medical': 0.0,\n    'surgical': 0.0,\n    'haem/onc': 0.0,\n    'paediatric': 1.0\n}\n\n# Function to determine if the patient is a child\nspecial_category_func = lambda row: row['age_on_arrival'] &lt; 18\n\nspecial_func_map = {\n    'paediatric': special_category_func,\n    'default': lambda row: True  # Default function for other specialties\n}\n\nprediction_time = (15,30)\nprediction_snapshots = pd.DataFrame([\n    {'age_on_arrival': 15, 'elapsed_los': 3600},\n    {'age_on_arrival': 45, 'elapsed_los': 7200}\n])\nspecialties = ['paediatric', 'medical']\nprediction_window_hrs = 4.0\ncdf_cut_points = [0.7, 0.9]\nx1, y1, x2, y2 = 4.0, 0.76, 12.0, 0.99\n\npredictions = create_predictions(\n    model_file_path='path/to/model/file',\n    prediction_time=prediction_time,\n    prediction_snapshots=prediction_snapshots,\n    specialties=specialties,\n    prediction_window_hrs=prediction_window_hrs,\n    cdf_cut_points=cdf_cut_points,\n    x1=x1,\n    y1=y1,\n    x2=x2,\n    y2=y2,\n    special_func_map=special_func_map,\n    special_category_dict=special_category_dict,\n    special_category_func=special_category_func\n\n\n)\n</code></pre> Source code in <code>src/patientflow/predict/realtime_demand.py</code> <pre><code>def create_predictions(\n    model_file_path: str,\n    prediction_time: Tuple,\n    prediction_snapshots: pd.DataFrame,\n    specialties: List[str],\n    prediction_window_hrs: float,\n    x1: float,\n    y1: float,\n    x2: float,\n    y2: float,\n    cdf_cut_points: List[float],\n    special_params: Optional[Dict[str, Any]] = None,\n) -&gt; Dict[str, Dict[str, List[int]]]:\n    \"\"\"\n    Create predictions for emergency demand for a single prediction moment.\n\n    Parameters:\n    - model_file_path (str): Path to the model files.\n    - prediction_moment (Tuple): Hour and minute of time for which model to be used for inference was trained\n    - prediction_snapshots (pd.DataFrame): DataFrame containing prediction snapshots.\n    - specialties (List[str]): List of specialty names for which predictions are required.\n    - prediction_window_hrs (float): Prediction window in hours.\n    - x1, y1, x2, y2 (float): Parameters for calculating probability of admission within prediction window.\n    - cdf_cut_points (List[float]): List of cumulative distribution function cut points.\n    - special_params (Optional[Dict[str, Any]]): Dictionary containing 'special_category_func', 'special_category_dict', and 'special_func_map'.\n      - special_category_func (Callable[[Any], Any]): Function identifying patients whose specialty predictions are handled outside the get_specialty_probs() function.\n      - special_category_dict (Dict[str, Any]): Dictionary of probabilities applied to those patients.\n      - special_func_map (Dict[str, Callable[[pd.Series], bool]]): A dictionary mapping specialties to specific functions that are applied to each row of the prediction snapshots to filter indices.\n\n    Returns:\n    - Dict[str, Dict[str, List[int]]]: Predictions for each specialty.\n\n    Example:\n    ```python\n    from datetime import datetime\n    import pandas as pd\n\n    special_category_dict = {\n        'medical': 0.0,\n        'surgical': 0.0,\n        'haem/onc': 0.0,\n        'paediatric': 1.0\n    }\n\n    # Function to determine if the patient is a child\n    special_category_func = lambda row: row['age_on_arrival'] &lt; 18\n\n    special_func_map = {\n        'paediatric': special_category_func,\n        'default': lambda row: True  # Default function for other specialties\n    }\n\n    prediction_time = (15,30)\n    prediction_snapshots = pd.DataFrame([\n        {'age_on_arrival': 15, 'elapsed_los': 3600},\n        {'age_on_arrival': 45, 'elapsed_los': 7200}\n    ])\n    specialties = ['paediatric', 'medical']\n    prediction_window_hrs = 4.0\n    cdf_cut_points = [0.7, 0.9]\n    x1, y1, x2, y2 = 4.0, 0.76, 12.0, 0.99\n\n    predictions = create_predictions(\n        model_file_path='path/to/model/file',\n        prediction_time=prediction_time,\n        prediction_snapshots=prediction_snapshots,\n        specialties=specialties,\n        prediction_window_hrs=prediction_window_hrs,\n        cdf_cut_points=cdf_cut_points,\n        x1=x1,\n        y1=y1,\n        x2=x2,\n        y2=y2,\n        special_func_map=special_func_map,\n        special_category_dict=special_category_dict,\n        special_category_func=special_category_func\n\n\n    )\n    ```\n    \"\"\"\n\n    if special_params:\n        validate_special_category_objects(special_params)\n        special_category_func = special_params[\"special_category_func\"]\n        special_category_dict = special_params[\"special_category_dict\"]\n        special_func_map = special_params[\"special_func_map\"]\n    else:\n        special_category_func = special_category_dict = special_func_map = None\n\n    predictions: Dict[str, Dict[str, List[int]]] = {\n        specialty: {\"in_ed\": [], \"yet_to_arrive\": []} for specialty in specialties\n    }\n\n    # Load models\n    admissions_model = prepare_for_inference(\n        model_file_path=model_file_path,\n        model_name=\"admissions\",\n        prediction_time=prediction_time,\n        model_only=True,\n    )\n\n    # add missing columns to predictions_snapshots that are expected by the model\n    prediction_snapshots = add_missing_columns(admissions_model, prediction_snapshots)\n\n    yet_to_arrive_model_name = (\n        f\"ed_yet_to_arrive_by_spec_{int(prediction_window_hrs)}_hours\"\n    )\n    yet_to_arrive_model = prepare_for_inference(\n        model_file_path=model_file_path,\n        model_name=yet_to_arrive_model_name,\n        model_only=True,\n    )\n\n    # Get predictions of admissions for ED patients\n    prob_admission_after_ed = model_input_to_pred_proba(\n        prediction_snapshots, admissions_model\n    )\n\n    # Get predictions of admission to specialty\n    prediction_snapshots.loc[:, \"specialty_prob\"] = get_specialty_probs(\n        model_file_path,\n        prediction_snapshots,\n        special_category_func=special_category_func,\n        special_category_dict=special_category_dict,\n    )\n\n    prediction_snapshots.loc[:, \"elapsed_los_hrs\"] = prediction_snapshots[\n        \"elapsed_los\"\n    ].apply(lambda x: x / 3600)\n\n    # Get probability of admission within prediction window\n    prob_admission_in_window = prediction_snapshots.apply(\n        lambda row: calculate_probability(\n            row[\"elapsed_los_hrs\"], prediction_window_hrs, x1, y1, x2, y2\n        ),\n        axis=1,\n    )\n\n    if special_func_map is None:\n        special_func_map = {\"default\": lambda row: True}\n\n    for specialty in specialties:\n        func = special_func_map.get(specialty, special_func_map[\"default\"])\n        non_zero_indices = prediction_snapshots[\n            prediction_snapshots.apply(func, axis=1)\n        ].index\n\n        filtered_prob_admission_after_ed = prob_admission_after_ed.loc[non_zero_indices]\n        prob_admission_to_specialty = prediction_snapshots[\"specialty_prob\"].apply(\n            lambda x: x[specialty]\n        )\n        filtered_prob_admission_to_specialty = prob_admission_to_specialty.loc[\n            non_zero_indices\n        ]\n        filtered_prob_admission_in_window = prob_admission_in_window.loc[\n            non_zero_indices\n        ]\n\n        filtered_weights = (\n            filtered_prob_admission_to_specialty * filtered_prob_admission_in_window\n        )\n        agg_predicted_in_ed = pred_proba_to_agg_predicted(\n            filtered_prob_admission_after_ed, weights=filtered_weights\n        )\n\n        prediction_context = {specialty: {\"prediction_time\": prediction_time}}\n        agg_predicted_yta = yet_to_arrive_model.predict(\n            prediction_context, x1, y1, x2, y2\n        )\n\n        predictions[specialty][\"in_ed\"] = [\n            index_of_sum(agg_predicted_in_ed[\"agg_proba\"].values.cumsum(), cut_point)\n            for cut_point in cdf_cut_points\n        ]\n        predictions[specialty][\"yet_to_arrive\"] = [\n            index_of_sum(\n                agg_predicted_yta[specialty][\"agg_proba\"].values.cumsum(), cut_point\n            )\n            for cut_point in cdf_cut_points\n        ]\n\n    return predictions\n</code></pre>"},{"location":"api/#patientflow.predict.realtime_demand.index_of_sum","title":"<code>index_of_sum(sequence, max_sum)</code>","text":"<p>Returns the index where the cumulative sum of a sequence of probabilities exceeds max_sum.</p> Source code in <code>src/patientflow/predict/realtime_demand.py</code> <pre><code>def index_of_sum(sequence: List[float], max_sum: float) -&gt; int:\n    \"\"\"Returns the index where the cumulative sum of a sequence of probabilities exceeds max_sum.\"\"\"\n    cumulative_sum = 0.0\n    for i, value in enumerate(sequence):\n        cumulative_sum += value\n        if cumulative_sum &gt;= 1 - max_sum:\n            return i\n    return len(sequence) - 1  # Return the last index if the sum doesn't exceed max_sum\n</code></pre>"},{"location":"api/#patientflow.predict.specialty_of_admission","title":"<code>specialty_of_admission</code>","text":"<p>Module for calculating specialty probability distributions for patient visits.</p> <p>This module provides a function <code>get_specialty_probs</code> that leverages a predictive model to compute specialty probability distributions for patient visits based on their data. It supports custom classification of certain visits into special categories that can have predefined probability distributions.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission--functions","title":"Functions","text":"<p>get_specialty_probs(model_file_path, snapshots_df, special_category_func=None, special_category_dict=None)     Calculate specialty probability distributions for patient visits based on their data.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission.get_specialty_probs","title":"<code>get_specialty_probs(model_file_path, snapshots_df, special_category_func=None, special_category_dict=None)</code>","text":"<p>Calculate specialty probability distributions for patient visits based on their data.</p> <p>This function applies a predictive model to each row of the input DataFrame to compute specialty probability distributions. Optionally, it can classify certain rows as belonging to a special category (like pediatric cases) based on a user-defined function, applying a fixed probability distribution for these cases.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission.get_specialty_probs--parameters","title":"Parameters","text":"<p>model_file_path : str     Path to the predictive model file. snapshots_df : pandas.DataFrame     DataFrame containing the data on which predictions are to be made. Must include     a 'consultation_sequence' column if no special_category_func is applied. special_category_func : callable, optional     A function that takes a DataFrame row (Series) as input and returns True if the row     belongs to a special category that requires a fixed probability distribution.     If not provided, no special categorization is applied. special_category_dict : dict, optional     A dictionary containing the fixed probability distribution for special category cases.     This dictionary is applied to rows identified by <code>special_category_func</code>. If     <code>special_category_func</code> is provided, this parameter must also be provided.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission.get_specialty_probs--returns","title":"Returns","text":"<p>pandas.Series     A Series containing dictionaries as values. Each dictionary represents the probability     distribution of specialties for each patient visit.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission.get_specialty_probs--raises","title":"Raises","text":"<p>ValueError     If <code>special_category_func</code> is provided but <code>special_category_dict</code> is None.</p>"},{"location":"api/#patientflow.predict.specialty_of_admission.get_specialty_probs--examples","title":"Examples","text":"<p>snapshots_df = pd.DataFrame({ ...     'consultation_sequence': [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]], ...     'age': [5, 40, 70] ... }) def pediatric_case(row): ...     return row['age'] &lt; 18 special_dist = {'pediatrics': 0.9, 'general': 0.1} get_specialty_probs('model.pkl', snapshots_df, pediatric_case, special_dist) 0    {'pediatrics': 0.9, 'general': 0.1} 1    {'cardiology': 0.7, 'general': 0.3} 2    {'neurology': 0.8, 'general': 0.2} dtype: object</p> Source code in <code>src/patientflow/predict/specialty_of_admission.py</code> <pre><code>def get_specialty_probs(\n    model_file_path,\n    snapshots_df,\n    special_category_func=None,\n    special_category_dict=None,\n):\n    \"\"\"\n    Calculate specialty probability distributions for patient visits based on their data.\n\n    This function applies a predictive model to each row of the input DataFrame to compute\n    specialty probability distributions. Optionally, it can classify certain rows as\n    belonging to a special category (like pediatric cases) based on a user-defined function,\n    applying a fixed probability distribution for these cases.\n\n    Parameters\n    ----------\n    model_file_path : str\n        Path to the predictive model file.\n    snapshots_df : pandas.DataFrame\n        DataFrame containing the data on which predictions are to be made. Must include\n        a 'consultation_sequence' column if no special_category_func is applied.\n    special_category_func : callable, optional\n        A function that takes a DataFrame row (Series) as input and returns True if the row\n        belongs to a special category that requires a fixed probability distribution.\n        If not provided, no special categorization is applied.\n    special_category_dict : dict, optional\n        A dictionary containing the fixed probability distribution for special category cases.\n        This dictionary is applied to rows identified by `special_category_func`. If\n        `special_category_func` is provided, this parameter must also be provided.\n\n    Returns\n    -------\n    pandas.Series\n        A Series containing dictionaries as values. Each dictionary represents the probability\n        distribution of specialties for each patient visit.\n\n    Raises\n    ------\n    ValueError\n        If `special_category_func` is provided but `special_category_dict` is None.\n\n    Examples\n    --------\n    &gt;&gt;&gt; snapshots_df = pd.DataFrame({\n    ...     'consultation_sequence': [[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]],\n    ...     'age': [5, 40, 70]\n    ... })\n    &gt;&gt;&gt; def pediatric_case(row):\n    ...     return row['age'] &lt; 18\n    &gt;&gt;&gt; special_dist = {'pediatrics': 0.9, 'general': 0.1}\n    &gt;&gt;&gt; get_specialty_probs('model.pkl', snapshots_df, pediatric_case, special_dist)\n    0    {'pediatrics': 0.9, 'general': 0.1}\n    1    {'cardiology': 0.7, 'general': 0.3}\n    2    {'neurology': 0.8, 'general': 0.2}\n    dtype: object\n    \"\"\"\n\n    # Convert consultation_sequence to tuple if not already a tuple\n    if len(snapshots_df[\"consultation_sequence\"]) &gt; 0 and not isinstance(\n        snapshots_df[\"consultation_sequence\"].iloc[0], tuple\n    ):\n        snapshots_df.loc[:, \"consultation_sequence\"] = snapshots_df[\n            \"consultation_sequence\"\n        ].apply(lambda x: tuple(x) if x else ())\n\n    if special_category_func and not special_category_dict:\n        raise ValueError(\n            \"special_category_dict must be provided if special_category_func is specified.\"\n        )\n\n    # Load model for specialty predictions\n    specialty_model = prepare_for_inference(\n        model_file_path, \"ed_specialty\", model_only=True\n    )\n\n    # Function to determine the specialty probabilities\n    def determine_specialty(row):\n        if special_category_func and special_category_func(row):\n            return special_category_dict\n        else:\n            return specialty_model.predict(row[\"consultation_sequence\"])\n\n    # Apply the determine_specialty function to each row\n    specialty_prob_series = snapshots_df.apply(determine_specialty, axis=1)\n\n    # Find all unique keys used in any dictionary within the series\n    all_keys = set().union(\n        *(d.keys() for d in specialty_prob_series if isinstance(d, dict))\n    )\n\n    # Ensure each dictionary contains all keys found, with default values of 0 for missing keys\n    specialty_prob_series = specialty_prob_series.apply(\n        lambda d: (\n            {key: d.get(key, 0) for key in all_keys} if isinstance(d, dict) else d\n        )\n    )\n\n    return specialty_prob_series\n</code></pre>"},{"location":"api/#patientflow.prepare","title":"<code>prepare</code>","text":"<p>Module for preparing data, loading models, and organizing snapshots for inference.</p> <p>This module provides functionality to load a trained model, prepare data for making predictions, calculate arrival rates, and organize snapshot data. It allows for selecting one snapshot per visit, filtering snapshots by prediction time, and mapping snapshot dates to corresponding indices.</p>"},{"location":"api/#patientflow.prepare--functions","title":"Functions","text":"<p>prepare_for_inference(model_file_path, model_name, prediction_time=None,                       model_only=False, df=None, data_path=None,                       single_snapshot_per_visit=True, index_column='snapshot_id',                       sort_columns=None, eval_columns=None,                       exclude_from_training_data=None)     Loads a model and prepares data for inference.</p> <p>select_one_snapshot_per_visit(df, visit_col, seed=42)     Selects one snapshot per visit based on a random number and returns the filtered DataFrame.</p> <p>get_snapshots_at_prediction_time(df, prediction_time_, exclude_columns, single_snapshot_per_visit=True)     Filters the DataFrame by prediction time and optionally selects one snapshot per visit.</p> <p>prepare_snapshots_dict(df, start_dt=None, end_dt=None)     Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.</p> <p>calculate_time_varying_arrival_rates(df, yta_time_interval)     Calculates the time-varying arrival rates for a dataset indexed by datetime.</p>"},{"location":"api/#patientflow.prepare.calculate_time_varying_arrival_rates","title":"<code>calculate_time_varying_arrival_rates(df, yta_time_interval)</code>","text":"<p>Calculate the time-varying arrival rates for a dataset indexed by datetime.</p> <p>This function computes the arrival rates for each time interval specified, across the entire date range present in the dataframe. The arrival rate is calculated as the number of entries in the dataframe for each time interval, divided by the number of days in the dataset's timespan.</p> <p>Parameters df (pandas.DataFrame): A DataFrame indexed by datetime, representing the data for which arrival rates are to be calculated. The index of the DataFrame should be of datetime type. yta_time_interval (int): The time interval, in minutes, for which the arrival rates are to be calculated. For example, if <code>yta_time_interval=60</code>, the function will calculate hourly arrival rates.</p> <p>Returns dict: A dictionary where the keys are the start times of each interval (as <code>datetime.time</code> objects), and the values are the corresponding arrival rates (as floats).</p> <p>Raises TypeError: If the index of the DataFrame is not a datetime index.</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def calculate_time_varying_arrival_rates(df, yta_time_interval):\n    \"\"\"\n    Calculate the time-varying arrival rates for a dataset indexed by datetime.\n\n    This function computes the arrival rates for each time interval specified, across the entire date range present in the dataframe. The arrival rate is calculated as the number of entries in the dataframe for each time interval, divided by the number of days in the dataset's timespan.\n\n    Parameters\n    df (pandas.DataFrame): A DataFrame indexed by datetime, representing the data for which arrival rates are to be calculated. The index of the DataFrame should be of datetime type.\n    yta_time_interval (int): The time interval, in minutes, for which the arrival rates are to be calculated. For example, if `yta_time_interval=60`, the function will calculate hourly arrival rates.\n\n    Returns\n    dict: A dictionary where the keys are the start times of each interval (as `datetime.time` objects), and the values are the corresponding arrival rates (as floats).\n\n    Raises\n    TypeError: If the index of the DataFrame is not a datetime index.\n\n    \"\"\"\n    # Validate that the DataFrame index is a datetime object\n    if not isinstance(df.index, pd.DatetimeIndex):\n        raise TypeError(\"The DataFrame index must be a DatetimeIndex.\")\n\n    # Determine the start and end date of the data\n    start_dt = df.index.min()\n    end_dt = df.index.max()\n\n    # Convert start and end times to datetime if they are not already\n    if not isinstance(start_dt, datetime):\n        start_dt = datetime.strptime(start_dt, \"%Y-%m-%d %H:%M:%S%z\")\n\n    if not isinstance(end_dt, datetime):\n        end_dt = datetime.strptime(end_dt, \"%Y-%m-%d %H:%M:%S%z\")\n\n    # Calculate the total number of days covered by the dataset\n    num_days = pd.Series(df.index.date).nunique()\n    print(\n        f\"Calculating time-varying arrival rates for data provided, which spans {num_days} unique dates\"\n    )\n\n    arrival_rates_dict = {}\n\n    # Initialize a time object to iterate through one day in the specified intervals\n    _start_datetime = datetime(1970, 1, 1, 0, 0, 0, 0)\n    _stop_datetime = _start_datetime + timedelta(days=1)\n\n    # Iterate over each interval in a single day to calculate the arrival rate\n    while _start_datetime != _stop_datetime:\n        _start_time = _start_datetime.time()\n        _end_time = (_start_datetime + timedelta(minutes=yta_time_interval)).time()\n\n        # Filter the dataframe for entries within the current time interval\n        _df = df.between_time(_start_time, _end_time, inclusive=\"left\")\n\n        # Calculate and store the arrival rate for the interval\n        arrival_rates_dict[_start_time] = _df.shape[0] / num_days\n\n        # Move to the next interval\n        _start_datetime = _start_datetime + timedelta(minutes=yta_time_interval)\n\n    return arrival_rates_dict\n</code></pre>"},{"location":"api/#patientflow.prepare.prepare_for_inference","title":"<code>prepare_for_inference(model_file_path, model_name, prediction_time=None, model_only=False, df=None, data_path=None, single_snapshot_per_visit=True, index_column='snapshot_id', sort_columns=['visit_number', 'snapshot_date', 'prediction_time'], eval_columns=['prediction_time', 'consultation_sequence', 'final_sequence'], exclude_from_training_data=['visit_number', 'snapshot_date', 'prediction_time'])</code>","text":"<p>Load a trained model and prepare data for making predictions.</p> <p>This function retrieves a trained model from a specified file path and, if requested, prepares the data required for inference. The data can be provided either as a DataFrame or as a file path to a CSV file. The function allows filtering and processing of the data to match the model's requirements.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--parameters","title":"Parameters","text":"<p>model_file_path : str     The file path where the trained model is saved. model_name : str     The name of the model to be loaded. prediction_time : str, optional     The time at which predictions are to be made. This is used to filter     the data for the relevant time snapshot. model_only : bool, optional     If True, only the model is returned. If False, both the prepared data     and the model are returned. Default is False. df : pandas.DataFrame, optional     The DataFrame containing the data to be used for inference. If not     provided, data_path must be specified. data_path : str, optional     The file path to a CSV file containing the data to be used for inference.     Ignored if <code>df</code> is provided. single_snapshot_per_visit : bool, optional     If True, only a single snapshot per visit is considered. Default is True. index_column : str, optional     The name of the index column in the data. Default is 'snapshot_id'. sort_columns : list of str, optional     The columns to sort the data by. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"]. eval_columns : list of str, optional     The columns that require literal evaluation of their content when loading from csv.     Default is [\"prediction_time\", \"consultation_sequence\", \"final_sequence\"]. exclude_from_training_data : list of str, optional     The columns to be excluded from the training data. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--returns","title":"Returns","text":"<p>model : object     The loaded model. X_test : pandas.DataFrame, optional     The features prepared for testing, returned only if model_only is False. y_test : pandas.Series, optional     The labels corresponding to X_test, returned only if model_only is False.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--raises","title":"Raises","text":"<p>KeyError     If the 'training_validation_test' column is not found in the provided DataFrame.</p>"},{"location":"api/#patientflow.prepare.prepare_for_inference--notes","title":"Notes","text":"<p>Either <code>df</code> or <code>data_path</code> must be provided. If neither is provided or if <code>df</code> is empty, the function will print an error message and return None.</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def prepare_for_inference(\n    model_file_path,\n    model_name,\n    prediction_time=None,\n    model_only=False,\n    df=None,\n    data_path=None,\n    single_snapshot_per_visit=True,\n    index_column=\"snapshot_id\",\n    sort_columns=[\"visit_number\", \"snapshot_date\", \"prediction_time\"],\n    eval_columns=[\"prediction_time\", \"consultation_sequence\", \"final_sequence\"],\n    exclude_from_training_data=[\"visit_number\", \"snapshot_date\", \"prediction_time\"],\n):\n    \"\"\"\n    Load a trained model and prepare data for making predictions.\n\n    This function retrieves a trained model from a specified file path and,\n    if requested, prepares the data required for inference. The data can be\n    provided either as a DataFrame or as a file path to a CSV file. The function\n    allows filtering and processing of the data to match the model's requirements.\n\n    Parameters\n    ----------\n    model_file_path : str\n        The file path where the trained model is saved.\n    model_name : str\n        The name of the model to be loaded.\n    prediction_time : str, optional\n        The time at which predictions are to be made. This is used to filter\n        the data for the relevant time snapshot.\n    model_only : bool, optional\n        If True, only the model is returned. If False, both the prepared data\n        and the model are returned. Default is False.\n    df : pandas.DataFrame, optional\n        The DataFrame containing the data to be used for inference. If not\n        provided, data_path must be specified.\n    data_path : str, optional\n        The file path to a CSV file containing the data to be used for inference.\n        Ignored if `df` is provided.\n    single_snapshot_per_visit : bool, optional\n        If True, only a single snapshot per visit is considered. Default is True.\n    index_column : str, optional\n        The name of the index column in the data. Default is 'snapshot_id'.\n    sort_columns : list of str, optional\n        The columns to sort the data by. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].\n    eval_columns : list of str, optional\n        The columns that require literal evaluation of their content when loading from csv.\n        Default is [\"prediction_time\", \"consultation_sequence\", \"final_sequence\"].\n    exclude_from_training_data : list of str, optional\n        The columns to be excluded from the training data. Default is [\"visit_number\", \"snapshot_date\", \"prediction_time\"].\n\n    Returns\n    -------\n    model : object\n        The loaded model.\n    X_test : pandas.DataFrame, optional\n        The features prepared for testing, returned only if model_only is False.\n    y_test : pandas.Series, optional\n        The labels corresponding to X_test, returned only if model_only is False.\n\n    Raises\n    ------\n    KeyError\n        If the 'training_validation_test' column is not found in the provided DataFrame.\n\n    Notes\n    -----\n    Either `df` or `data_path` must be provided. If neither is provided or if `df`\n    is empty, the function will print an error message and return None.\n\n    \"\"\"\n\n    # retrieve model trained for this time of day\n    model = load_saved_model(model_file_path, model_name, prediction_time)\n\n    if model_only:\n        return model\n\n    if data_path:\n        df = data_from_csv(data_path, index_column, sort_columns, eval_columns)\n    elif df is None or df.empty:\n        print(\"Please supply a dataset if not passing a data path\")\n        return None\n\n    try:\n        test_df = (\n            df[df.training_validation_test == \"test\"]\n            .drop(columns=\"training_validation_test\")\n            .copy()\n        )\n    except KeyError:\n        print(\"Column training_validation_test not found in dataframe\")\n        return None\n\n    X_test, y_test = get_snapshots_at_prediction_time(\n        test_df,\n        prediction_time,\n        exclude_from_training_data,\n        single_snapshot_per_visit,\n    )\n\n    return X_test, y_test, model\n</code></pre>"},{"location":"api/#patientflow.prepare.prepare_snapshots_dict","title":"<code>prepare_snapshots_dict(df, start_dt=None, end_dt=None)</code>","text":"<p>Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.</p> <p>Args: df (pd.DataFrame): DataFrame containing at least a 'snapshot_date' column which represents the dates. start_dt (datetime.date): Start date (optional) end_dt (datetime.date): End date (optional)</p> <p>Returns: dict: A dictionary where keys are dates and values are arrays of indices corresponding to each date's snapshots. A array can be empty if there are no snapshots associated with a date</p> Source code in <code>src/patientflow/prepare.py</code> <pre><code>def prepare_snapshots_dict(df, start_dt=None, end_dt=None):\n    \"\"\"\n    Prepares a dictionary mapping snapshot dates to their corresponding snapshot indices.\n\n    Args:\n    df (pd.DataFrame): DataFrame containing at least a 'snapshot_date' column which represents the dates.\n    start_dt (datetime.date): Start date (optional)\n    end_dt (datetime.date): End date (optional)\n\n    Returns:\n    dict: A dictionary where keys are dates and values are arrays of indices corresponding to each date's snapshots.\n    A array can be empty if there are no snapshots associated with a date\n\n    \"\"\"\n    # Ensure 'snapshot_date' is in the DataFrame\n    if \"snapshot_date\" not in df.columns:\n        raise ValueError(\"DataFrame must include a 'snapshot_date' column\")\n\n    # Group the DataFrame by 'snapshot_date' and collect the indices for each group\n    snapshots_dict = {\n        date: group.index.tolist() for date, group in df.groupby(\"snapshot_date\")\n    }\n\n    # If start_dt and end_dt are specified, add any missing keys from prediction_dates\n    if start_dt:\n        prediction_dates = pd.date_range(\n            start=start_dt, end=end_dt, freq=\"D\"\n        ).date.tolist()[:-1]\n        for dt in prediction_dates:\n            if dt not in snapshots_dict:\n                snapshots_dict[dt] = []\n\n    return snapshots_dict\n</code></pre>"},{"location":"api/#patientflow.train","title":"<code>train</code>","text":""},{"location":"api/#patientflow.train.chronological_cross_validation","title":"<code>chronological_cross_validation(pipeline, X, y, n_splits=5)</code>","text":"<p>Perform time series cross-validation.</p> <p>:param pipeline: The machine learning pipeline (preprocessing + model). :param X: Input features. :param y: Target variable. :param n_splits: Number of splits for cross-validation. :return: Dictionary with the average training and validation scores.</p> Source code in <code>src/patientflow/train.py</code> <pre><code>def chronological_cross_validation(pipeline, X, y, n_splits=5):\n    \"\"\"\n    Perform time series cross-validation.\n\n    :param pipeline: The machine learning pipeline (preprocessing + model).\n    :param X: Input features.\n    :param y: Target variable.\n    :param n_splits: Number of splits for cross-validation.\n    :return: Dictionary with the average training and validation scores.\n    \"\"\"\n    # Initialize TimeSeriesSplit\n    tscv = TimeSeriesSplit(n_splits=n_splits)\n\n    # Lists to collect scores for each fold\n    train_aucs = []\n    train_loglosses = []\n    valid_aucs = []\n    valid_loglosses = []\n\n    # Iterate over train-test splits\n    for train_index, test_index in tscv.split(X):\n        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n\n        # Fit the pipeline to the training data\n        # Note that you don't need to manually transform the data; the pipeline handles it\n        pipeline.fit(X_train, y_train)\n\n        # # To access transformed feature names:\n        # transformed_cols = pipeline.named_steps['feature_transformer'].get_feature_names_out()\n        # transformed_cols = [col.split('__')[-1] for col in transformed_cols]\n\n        # Evaluate on the training split\n        y_train_pred = pipeline.predict_proba(X_train)[:, 1]\n        train_auc = roc_auc_score(y_train, y_train_pred)\n        train_logloss = log_loss(y_train, y_train_pred)\n        train_aucs.append(train_auc)\n        train_loglosses.append(train_logloss)\n\n        # Evaluate on the validation split\n        y_valid_pred = pipeline.predict_proba(X_valid)[:, 1]\n        valid_auc = roc_auc_score(y_valid, y_valid_pred)\n        valid_logloss = log_loss(y_valid, y_valid_pred)\n        valid_aucs.append(valid_auc)\n        valid_loglosses.append(valid_logloss)\n\n    # Calculate mean scores\n    mean_train_auc = sum(train_aucs) / n_splits\n    mean_train_logloss = sum(train_loglosses) / n_splits\n    mean_valid_auc = sum(valid_aucs) / n_splits\n    mean_valid_logloss = sum(valid_loglosses) / n_splits\n\n    return {\n        \"train_auc\": mean_train_auc,\n        \"valid_auc\": mean_valid_auc,\n        \"train_logloss\": mean_train_logloss,\n        \"valid_logloss\": mean_valid_logloss,\n    }\n</code></pre>"},{"location":"api/#patientflow.train.create_column_transformer","title":"<code>create_column_transformer(df, ordinal_mappings=None)</code>","text":"<p>Create a column transformer for a dataframe with dynamic column handling.</p> <p>:param df: Input dataframe. :param ordinal_mappings: A dictionary specifying the ordinal mappings for specific columns. :return: A configured ColumnTransformer object.</p> Source code in <code>src/patientflow/train.py</code> <pre><code>def create_column_transformer(df, ordinal_mappings=None):\n    \"\"\"\n    Create a column transformer for a dataframe with dynamic column handling.\n\n    :param df: Input dataframe.\n    :param ordinal_mappings: A dictionary specifying the ordinal mappings for specific columns.\n    :return: A configured ColumnTransformer object.\n    \"\"\"\n    transformers = []\n\n    # Default to an empty dict if no ordinal mappings are provided\n    if ordinal_mappings is None:\n        ordinal_mappings = {}\n\n    for col in df.columns:\n        if col in ordinal_mappings:\n            # Ordinal encoding for specified columns with a predefined ordering\n            transformers.append(\n                (\n                    col,\n                    OrdinalEncoder(\n                        categories=[ordinal_mappings[col]],\n                        handle_unknown=\"use_encoded_value\",\n                        unknown_value=np.nan,\n                    ),\n                    [col],\n                )\n            )\n        elif df[col].dtype == \"object\" or (\n            df[col].dtype == \"bool\" or df[col].nunique() == 2\n        ):\n            # OneHotEncoding for categorical or boolean columns\n            transformers.append((col, OneHotEncoder(handle_unknown=\"ignore\"), [col]))\n        else:\n            # Scaling for numerical columns\n            transformers.append((col, StandardScaler(), [col]))\n\n    return ColumnTransformer(transformers)\n</code></pre>"},{"location":"notebooks/","title":"README","text":""},{"location":"notebooks/#about-the-notebooks","title":"About the notebooks","text":"<p>This folder contains a series of notebooks that demonstrate the process of modeling emergency demand in healthcare through a structured approach. Notebooks combine commentary with code and the results produced by that code. Here\u2019s how different audiences can benefit from these notebooks:</p> <ul> <li>For non-programmers seeking to understand the approach: If you're not familiar with programming, you can read the narrative sections of the notebooks as if it were a blog post to understand the strategies employed and skip the code snippets.</li> <li>For those new to Python or looking to learn: If you have some coding experience but are new to Python, the code snippets, combined with the narrative, provide insights into how Python is applied in modeling emergency bed demand. This includes tasks such as data exploration, model fitting, and result evaluation. Observing the output will help you see the practical outcomes of the modeling.</li> <li>For those interested in using the 'patientflow' package: These notebooks serve as a guide on how to use the 'patientflow' package. Through walkthroughs of the functions and their applications, you can learn how to integrate this package into your own projects.</li> </ul>"},{"location":"notebooks/#outline-of-the-notebooks","title":"Outline of the notebooks","text":"<ul> <li>1_Introduce_our_users: Talks about the users of emergency demand predictions in acute hospitals.</li> <li>2_Modelling_requirements: Explains design choices that were made to develop a practical model, and shows an example of the output that is sent five times a day at UCLH.</li> <li>3_Introduce_the_datasets: Introduces the two datasets created to accompany this repository</li> <li>4a_Predict_probability_of_admission_from_ED: Shows how to train a machine learning model to forecast admission likelihood based on patient data from the Emergency Department (ED). This includes dividing the data into training, validation, and testing sets, as well as into subsets based on the time of day the predictions are made, applying an XGBoost model for predictions, and saving the models for future use.</li> <li>4b_Predict_demand_from_patients_in_ED Illustrates how to convert individual admission probabilities into an overall bed demand forecast</li> <li>4c_Predict_probablity_of_admission_to_specialty: Shows how to train a model predicting specialty of admission; a sequence of consultation requests is mapped to a probability of being admitted to one of three specialties: medical, surgical, haematology/oncology, with paediatric patients (under 18) handled differently</li> <li>4d_Predict_demand_from_patients_yet_to_arrive: Show the use of a time-varying weighted Poisson distribution to predict a number of patients yet to arrive with a prediction window (say 8 hours) of the time of prediction, by specialty. Demonstrates the use of a function that will take ED performance targets into account when predicting the number admitted by the end of the prediction window</li> <li>5_Model_evaluation: Discusses how to evaluate the models' predictions</li> <li>6 Bring it all together: Shows an example of doing live inference using the models trained in the previous steps</li> </ul>"},{"location":"src/patientflow/","title":"README","text":""},{"location":"src/patientflow/#patientflow-a-forthcoming-python-package","title":"PatientFlow: A forthcoming Python package","text":"<p>Our intention is to release this repository as a Python package that can be installed using common methods like <code>pip install</code></p> <p>The package will support predictions of bed demand and discharges by providing functions that</p> <ul> <li>predict patient-level probabilities of admission and discharge, by specialty</li> <li>create probability distributions predicting number of beds needed for or vacated by those patients, at different levels of aggregation</li> <li>return a net bed position by combining predictions of demand and supply of beds</li> <li>evaluate and provide visualisation of the performance of these predictions</li> </ul> <p>The package is intended to serve as a wrapper of the functions typically used for such purposes in the <code>sklearn</code> and <code>scipy</code> python packages, with additional context to support their application and evaluation in bed management in healthcare</p>"},{"location":"src/patientflow/#modules-overview-in-order-of-their-use-in-a-typical-modelling-workflow","title":"Modules Overview (in order of their use in a typical modelling workflow)","text":"<ul> <li><code>load</code>: A module for loading configuration files, saved data and trained models</li> <li><code>prepare</code>: A module for preparing saved data prior to input into model training</li> <li><code>train</code>: A module and submodules for training predictive models</li> <li><code>predictors</code>: A module and submodules containing custom predictors developed for the <code>patientflow</code> package</li> <li><code>predict</code>: A module using trained models for predicting various aspects of bed demand and discharges</li> <li><code>aggregate</code>: A module that turns patient-level probabilities into aggregate distributions of bed numbers</li> <li><code>viz</code>: A module containing convenient plotting functions to examine the outputs from the above functions</li> </ul> <p>Other modules may follow in future</p>"},{"location":"src/patientflow/#deployment","title":"Deployment","text":"<p>This package is designed for use in hospital data projects analysing patient flow and bed capacity in short time horizons. The modules can be customised to align with specific hospital requirements</p>"}]}