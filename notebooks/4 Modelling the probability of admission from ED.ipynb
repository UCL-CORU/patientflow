{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Predict ED admission probability\n",
                "\n",
                "This notebook demonstrates the first stage of prediction, to generate a probability of admission for each patient in the ED. \n",
                "\n",
                "As one of the modelling decisions is to send predictions at specified times of day, we tailor the models to these times and train one model for each time. The dataset used for this modelling is derived from snapshots of visits at each time of day. The times of day are define in config.json file in the root directory of this repo. \n",
                "\n",
                "A patient episode (visit) may well span more than one of these times, so we need to consider how we will deal with the occurence of multiple snapshots per episode. At each of these times of day, we will use only one training sample from each hospital episode.\n",
                "\n",
                "Separation of the visits into training, validation and test sets will be done chronologically into a training, validation and test set \n",
                "\n",
                "Evaluation of individual level models includes: \n",
                "- feature importance plots\n",
                "- calibration plot\n",
                "- MADCAP overall, plus breakdown by age category and length of stay\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set up the notebook environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Reload functions every time\n",
                "%load_ext autoreload \n",
                "%autoreload 2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import sys\n",
                "import json\n",
                "\n",
                "\n",
                "PROJECT_ROOT = Path().home() / 'HyMind'\n",
                "\n",
                "# Patient flow package\n",
                "USER_ROOT = Path().home()\n",
                "sys.path.append(str(USER_ROOT / 'patientflow' / 'patientflow' ))\n",
                "\n",
                "# Functions that sit outside the package\n",
                "sys.path.append(str(USER_ROOT / 'patientflow' / 'functions' ))\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_file_path = PROJECT_ROOT / 'dissemination' / 'model-output' / 'trained-models'\n",
                "model_file_path\n",
                "\n",
                "data_file_path = PROJECT_ROOT / 'dissemination' / 'data-raw'\n",
                "data_file_path"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sys.path"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load parameters\n",
                "\n",
                "These are set in config.json. You can change these for your own purposes. But the times of day will need to match those in the provided dataset if you want to run this notebook successfully."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'yaml'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the times of day\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      4\u001b[0m config_path \u001b[38;5;241m=\u001b[39m Path(PROJECT_ROOT \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdissemination\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(config_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n",
                        "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
                    ]
                }
            ],
            "source": [
                "# Load the times of day\n",
                "import yaml\n",
                "\n",
                "config_path = Path(PROJECT_ROOT / 'dissemination')\n",
                "\n",
                "with open(config_path / 'config.yaml', 'r') as file:\n",
                "    config = yaml.safe_load(file)\n",
                "    \n",
                "# Convert list of times of day at which predictions will be made (currently stored as lists) to list of tuples\n",
                "prediction_times = [tuple(item) for item in config['prediction_times']]\n",
                "\n",
                "# See the times of day at which predictions will be made\n",
                "prediction_times"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load data"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from ed_admissions_data_retrieval import ed_admissions_get_data\n",
                "PATH_ED = 'HyMind/dissemination/data-raw/ED_visits.csv'\n",
                "\n",
                "df = ed_admissions_get_data(PATH_ED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "See how many visits there are at each time of day in the dataset. We see that number of visits represented is greater in the afternoon and evening"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(df.prediction_time.value_counts())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will confirm that the dataset aligns with the specified times of day set in the parameters file config.yaml. That is because, later, we will use these times of day to evaluate the predictions. The evaluation will fail if the data loaded does not match. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"\\nTimes of day at which predictions will be made\")\n",
                "print(prediction_times)\n",
                "print(\"\\nNumber of rows in dataset that are not in these times of day\")\n",
                "print(len(df[~df.prediction_time.isin(prediction_times)]))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Set an index column in df\n",
                "\n",
                "Setting the index as the snapshot_id before subsetting means that we retain the same values of snapshot_id throughout the entire process, ensuring that they are consistent across the original dataset df and the training, validation and test subsets of df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "After executing the code below, the snapshot_id has been set as the index column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if df.index.name != 'snapshot_id':\n",
                "    df = df.set_index('snapshot_id')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Separate into training, validation and test sets\n",
                "\n",
                "As part of preparing the data, each visit has already been allocated into one of three sets - training, vaidation and test sets. This has been done chronologically, as shown by the output below. Using a chronological approach is appropriate for tasks where the model needs to be validated on unseen, future data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for value in df.training_validation_test.unique():\n",
                "    subset = df[df.training_validation_test == value]\n",
                "    counts = subset.training_validation_test.value_counts().values[0]\n",
                "    min_date = subset.snapshot_datetime.min()\n",
                "    max_date = subset.snapshot_datetime.max()\n",
                "    print(f\"Set: {value}\\nNumber of rows: {counts}\\nMin Date: {min_date}\\nMax Date: {max_date}\\n\")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df = df[df.training_validation_test == 'train'].drop(columns='training_validation_test')\n",
                "valid_df = df[df.training_validation_test == 'valid'].drop(columns='training_validation_test')\n",
                "test_df = df[df.training_validation_test == 'test'].drop(columns='training_validation_test')\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We can see below that some visits appear more than once in each of these sets. (No visit appears in more than one set.)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df.visit_number.value_counts()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "For example, the below patient has 35 episode slices, of which the majority are in the ED location 'OTF'. OTF refers to 'Off the Floor'. Patients can sometimes be moved to this location when paperwork is due, while in fact the patient has already left the ED. While it is tempting to remove these OTF locations, in real-time these patients would be picked up, so a model would ideally be trained on this data. Therefore we do need to include them in our training set. "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_df[train_df.visit_number == 68031].head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Train a XGBoost Classifier for each time of day, and save the best model"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The first step is to load a transformer for the ML training data to turn it into a format that our ML classifier can read. This is done using a function called create_column_transformer() which called ColumnTransfomer() a standard method in scikit-learn. This function could be changed for different input\n",
                "\n",
                "The ColumnTransformer in scikit-learn is a tool that applies different transformations or preprocessing steps to different columns of a dataset in a single operation. OneHotEncoder converts categorical data into a format that can be provided to machine learning algorithms; without this, the model might interpret the categorical data as numerical, which would lead to incorrect results. With the OrdinalEncoder, categories are converted into ordered numerical values to reflect the inherent order in the age groups\n",
                "\n",
                "We can also specify a grid of hyperparameters, so that the classifier will iterate though them to find the best fitting model. \n",
                "\n",
                "We are interested in predictions at different times of day. So we will train a model for each time of day. We will filter each visit so that it only appears once in the training data. A random number has already been included in the dataset to facilitate this.\n",
                "\n",
                "We then iterate through the grid to find the best model for each time of day, keeping track of the best model and its results. \n",
                "\n",
                "The best model is saved, plus a dictionary of its metadata, including\n",
                "\n",
                "* how many visits were in training, validation and test sets\n",
                "* Area under ROC curve and log loss (performance metrics) for training (based on 5-fold cross validation), validation and test sets\n",
                "* List of features and their importances in the model\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Function for cross validation\n",
                "\n",
                "The ML models will be trained across a range of different hyperparameter options. When evaluating the best model, we will save common ML metrics (AUC and logloss) and compare each model for the best (lowest) logloss. Apply a chronological approach to the cross-validation split is appropriate for tasks where the model needs to be validated on unseen, future data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "from sklearn.metrics import roc_auc_score, log_loss\n",
                "\n",
                "\n",
                "\n",
                "def chronological_cross_validation(pipeline, X, y, n_splits=5):\n",
                "    \"\"\"\n",
                "    Perform time series cross-validation.\n",
                "\n",
                "    :param pipeline: The machine learning pipeline (preprocessing + model).\n",
                "    :param X: Input features.\n",
                "    :param y: Target variable.\n",
                "    :param n_splits: Number of splits for cross-validation.\n",
                "    :return: Dictionary with the average training and validation scores.\n",
                "    \"\"\"\n",
                "    # Initialize TimeSeriesSplit\n",
                "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
                "\n",
                "    # Lists to collect scores for each fold\n",
                "    train_aucs = []\n",
                "    train_loglosses = []\n",
                "    valid_aucs = []\n",
                "    valid_loglosses = []\n",
                "\n",
                "    # Iterate over train-test splits\n",
                "    for train_index, test_index in tscv.split(X):\n",
                "        X_train, X_valid = X.iloc[train_index], X.iloc[test_index]\n",
                "        y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
                "\n",
                "        # Fit the pipeline to the training data\n",
                "        # Note that you don't need to manually transform the data; the pipeline handles it\n",
                "        pipeline.fit(X_train, y_train)\n",
                "        \n",
                "        # # To access transformed feature names:\n",
                "        # transformed_cols = pipeline.named_steps['feature_transformer'].get_feature_names_out()\n",
                "        # transformed_cols = [col.split('__')[-1] for col in transformed_cols]\n",
                "\n",
                "        # Evaluate on the training split\n",
                "        y_train_pred = pipeline.predict_proba(X_train)[:, 1]\n",
                "        train_auc = roc_auc_score(y_train, y_train_pred)\n",
                "        train_logloss = log_loss(y_train, y_train_pred)\n",
                "        train_aucs.append(train_auc)\n",
                "        train_loglosses.append(train_logloss)\n",
                "\n",
                "        # Evaluate on the validation split\n",
                "        y_valid_pred = pipeline.predict_proba(X_valid)[:, 1]\n",
                "        valid_auc = roc_auc_score(y_valid, y_valid_pred)\n",
                "        valid_logloss = log_loss(y_valid, y_valid_pred)\n",
                "        valid_aucs.append(valid_auc)\n",
                "        valid_loglosses.append(valid_logloss)\n",
                "\n",
                "    # Calculate mean scores\n",
                "    mean_train_auc = sum(train_aucs) / n_splits\n",
                "    mean_train_logloss = sum(train_loglosses) / n_splits\n",
                "    mean_valid_auc = sum(valid_aucs) / n_splits\n",
                "    mean_valid_logloss = sum(valid_loglosses) / n_splits\n",
                "\n",
                "    return {\n",
                "        'train_auc': mean_train_auc,\n",
                "        'valid_auc': mean_valid_auc,\n",
                "        'train_logloss': mean_train_logloss,\n",
                "        'valid_logloss': mean_valid_logloss,\n",
                "    }\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Initialise the model with given hyperparameters\n",
                "def initialise_model(params):\n",
                "    model = xgb.XGBClassifier(n_jobs=-1, use_label_encoder=False, eval_metric=\"logloss\")\n",
                "    model.set_params(**params)\n",
                "    return model\n",
                "\n",
                "\n",
                "# Set up the feature transformation\n",
                "def create_column_transformer(df, ordinal_mappings=None):\n",
                "    \"\"\"\n",
                "    Create a column transformer for a dataframe with dynamic column handling.\n",
                "\n",
                "    :param df: Input dataframe.\n",
                "    :param ordinal_mappings: A dictionary specifying the ordinal mappings for specific columns.\n",
                "    :return: A configured ColumnTransformer object.\n",
                "    \"\"\"\n",
                "    transformers = []\n",
                "    \n",
                "    # Default to an empty dict if no ordinal mappings are provided\n",
                "    if ordinal_mappings is None:\n",
                "        ordinal_mappings = {}\n",
                "\n",
                "    for col in df.columns:\n",
                "        if col in ordinal_mappings:\n",
                "            # Ordinal encoding for specified columns with a predefined ordering\n",
                "            transformers.append((col, OrdinalEncoder(categories=[ordinal_mappings[col]],\n",
                "                                                     handle_unknown='use_encoded_value', unknown_value=np.nan), [col]))\n",
                "        elif df[col].dtype == 'object' or (df[col].dtype == 'bool' or df[col].nunique() == 2):\n",
                "        # OneHotEncoding for categorical or boolean columns\n",
                "            transformers.append((col, OneHotEncoder(handle_unknown='ignore'), [col]))\n",
                "        else:\n",
                "            # Passthrough for other types of columns (e.g., numerical)\n",
                "            transformers.append((col, 'passthrough', [col]))\n",
                "\n",
                "    return ColumnTransformer(transformers)\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "\n",
                "import xgboost as xgb\n",
                "\n",
                "from sklearn.model_selection import ParameterGrid, cross_validate\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
                "from sklearn.model_selection import TimeSeriesSplit\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix,\n",
                "    ConfusionMatrixDisplay,\n",
                "    log_loss,\n",
                "    roc_auc_score,\n",
                ")\n",
                "from joblib import dump, load\n",
                "\n",
                "from ed_admissions_utils import get_model_name, preprocess_data\n",
                "\n",
                "# initialize a dict to save information about the best models for each time of day\n",
                "best_model_results_dict = {}\n",
                "\n",
                "# Specify where to save the dict and the trained models\n",
                "model_file_path = PROJECT_ROOT / 'dissemination' / 'model-output' \n",
                "\n",
                "# Option to iterate through different hyperparameters for XGBoost\n",
                "grid = {\n",
                "    'n_estimators':[30, 40, 50],\n",
                "    'subsample':[0.7,0.8,0.9],\n",
                "    'colsample_bytree': [0.7,0.8,0.9]\n",
                "}\n",
                "\n",
                "# certain columns are not used in training\n",
                "exclude_from_training_data = [\n",
                "    \"visit_number\",\n",
                "    \"snapshot_datetime\",\n",
                "    \"prediction_time\"]\n",
                "\n",
                "\n",
                "ordinal_mappings = {\n",
                "    \"age_group\": [\"0-17\", \"18-24\", \"25-34\", \"35-44\", \"45-54\", \"55-64\", \"65-74\", \"75-102\"],\n",
                "    \"latest_acvpu\": [\"A\", \"C\", \"V\", \"P\", \"U\"],\n",
                "    \"latest_manch_triage\": [\"Blue\", \"Green\", \"Yellow\", \"Orange\", \"Red\"],\n",
                "    \"latest_pain_objective\": [\"Nil\", \"Mild\", \"Moderate\", \"Severe\\\\Very Severe\"]\n",
                "}\n",
                "\n",
                "\n",
                "# Process each time of day\n",
                "for prediction_time_ in prediction_times:\n",
                "\n",
                "    print(\"\\nProcessing :\" + str(prediction_time_))\n",
                "\n",
                "    # create a name for the model based on the time of day it is trained for\n",
                "    MODEL__ED_ADMISSIONS__NAME = get_model_name(prediction_time_)\n",
                "\n",
                "    # use this name in the path for saving best model\n",
                "    full_path = model_file_path / MODEL__ED_ADMISSIONS__NAME \n",
                "    full_path = full_path.with_suffix('.joblib')\n",
                "\n",
                "    # initialise data used for saving attributes of the model\n",
                "    best_model_results_dict[MODEL__ED_ADMISSIONS__NAME] = {}\n",
                "    best_valid_logloss = float('inf')\n",
                "    results_dict = {}\n",
                "    \n",
                "    # get visits that were in at the time of day in question and preprocess the training, validation and test sets \n",
                "    X_train, y_train = preprocess_data(train_df, prediction_time_, exclude_from_training_data)\n",
                "    X_valid, y_valid = preprocess_data(valid_df, prediction_time_, exclude_from_training_data)\n",
                "    X_test, y_test = preprocess_data(test_df, prediction_time_, exclude_from_training_data)\n",
                "    \n",
                "    # save size of each set\n",
                "    best_model_results_dict[MODEL__ED_ADMISSIONS__NAME]['train_valid_test_set_no'] = {\n",
                "        'train_set_no' : len(X_train),\n",
                "        'valid_set_no' : len(X_valid),\n",
                "        'test_set_no' : len(X_test),\n",
                "    }\n",
                "\n",
                "    # iterate through the grid of hyperparameters\n",
                "    for g in ParameterGrid(grid):\n",
                "        model = initialise_model(g)\n",
                "        \n",
                "        # define a column transformer for the ordinal and categorical variables\n",
                "        column_transformer = create_column_transformer(X_test)\n",
                "        \n",
                "        # create a pipeline with the feature transformer and the model\n",
                "        pipeline = Pipeline([\n",
                "            ('feature_transformer', column_transformer),\n",
                "            ('classifier', m)\n",
                "        ])\n",
                "\n",
                "        # cross-validate on training set using the function created earlier\n",
                "        cv_results = chronological_cross_validation(pipeline, X_train, y_train, n_splits=5)\n",
                "\n",
                "        # Store results for this set of parameters in the results dictionary\n",
                "        results_dict[str(g)] = {\n",
                "            'train_auc': cv_results['train_auc'],\n",
                "            'valid_auc': cv_results['valid_auc'],\n",
                "            'train_logloss': cv_results['train_logloss'],\n",
                "            'valid_logloss': cv_results['valid_logloss'],\n",
                "        }\n",
                "        \n",
                "        # Update and save best model if current model is better on validation set\n",
                "        if cv_results['valid_logloss'] < best_valid_logloss:\n",
                "\n",
                "            # save the details of the best model\n",
                "            best_model = str(g)\n",
                "            best_valid_logloss = cv_results['valid_logloss']\n",
                "\n",
                "            # save the best model params\n",
                "            best_model_results_dict[MODEL__ED_ADMISSIONS__NAME]['best_params'] = str(g)\n",
                "\n",
                "            # save the model metrics on training and validation set\n",
                "            best_model_results_dict[MODEL__ED_ADMISSIONS__NAME]['train_valid_set_results'] = results_dict\n",
                "\n",
                "            # score the model's performance on the test set  \n",
                "            y_test_pred_proba = pipeline.predict_proba(X_test)[:, 1]\n",
                "            test_auc = roc_auc_score(y_test, y_test_pred_proba)\n",
                "            test_logloss = log_loss(y_test,y_test_pred_proba)\n",
                "        \n",
                "            best_model_results_dict[MODEL__ED_ADMISSIONS__NAME]['test_set_results'] = {\n",
                "                'test_auc' : test_auc,\n",
                "                'test_logloss' : test_logloss\n",
                "            }\n",
                "\n",
                "            # save the best features\n",
                "            # To access transformed feature names:\n",
                "            transformed_cols = pipeline.named_steps['feature_transformer'].get_feature_names_out()\n",
                "            transformed_cols = [col.split('__')[-1] for col in transformed_cols]\n",
                "            best_model_results_dict[MODEL__ED_ADMISSIONS__NAME]['best_model_features'] = {\n",
                "                    'feature_names': transformed_cols,\n",
                "                    'feature_importances': pipeline.named_steps['classifier'].feature_importances_.tolist()\n",
                "                }\n",
                "\n",
                "            # save the best model\n",
                "            dump(pipeline, full_path)\n",
                "\n",
                "# save the results dictionary      \n",
                "filename_results_dict = 'best_model_results_dict.json'\n",
                "full_path_results_dict = model_file_path / filename_results_dict\n",
                "\n",
                "with open(full_path_results_dict, 'w') as f:\n",
                "    json.dump(best_model_results_dict, f)  "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# with corrected code\n",
                "for key, value in best_model_results_dict.items():\n",
                "    print(f\"Model: {key}; AUC: {round(value['test_set_results']['test_auc'],3)}; log loss {round(value['test_set_results']['test_logloss'],3)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# before corrected code\n",
                "for key, value in best_model_results_dict.items():\n",
                "    print(f\"Model: {key}; AUC: {round(value['test_set_results']['test_auc'],3)}; log loss {round(value['test_set_results']['test_logloss'],3)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "for key, value in best_model_results_dict.items():\n",
                "    print(f\"Model: {key}; AUC: {round(value['test_set_results']['test_auc'],3)}; log loss {round(value['test_set_results']['test_logloss'],3)}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "env",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
