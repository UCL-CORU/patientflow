{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "396817a6-13f3-4d44-939f-09c6bce18093",
   "metadata": {},
   "source": [
    "# WORK IM PROGRESS Convert EHR data to format used in this repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4762faa-7952-4494-ac74-e508edc49d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to EMAP\n",
    "# retrieve bed moves\n",
    "# identify moment of exit from ED/SDEC\n",
    "# set up snapshot datetimes to sample\n",
    "# sample ED visits at those times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a2f5ae-7eb3-4c2e-9b6d-250d9172926e",
   "metadata": {},
   "source": [
    "## DB connection script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144c7374-5266-47ee-86b1-504a4d54e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of how to set up database connection (NOT RUN)\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "def get_credentials():\n",
    "    with open('../../secret-emap', 'r') as file:\n",
    "        username = file.readline().strip()\n",
    "        password = file.readline().strip()\n",
    "        database_host = file.readline().strip()\n",
    "        database_name = file.readline().strip()\n",
    "        database_port = file.readline().strip()\n",
    "        return username, password, database_host, database_name, database_port\n",
    "\n",
    "# Get credentials from secret file\n",
    "username, password, database_host, database_name, database_port = get_credentials()\n",
    "\n",
    "# Database connection URL\n",
    "DATABASE_URL = f\"postgresql://{username}:{password}@{database_host}:{database_port}/{database_name}\"\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd87e869-6bf3-4fce-a545-8cde2dc589c7",
   "metadata": {},
   "source": [
    "## Retrieve the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7fc21-dd69-4c81-91ce-a922aa41dba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to mask the encounter numbers\n",
    "import hashlib\n",
    "\n",
    "def hash_csn(df):\n",
    "    \"\"\"\n",
    "    Consistently hash CSN values in a dataframe\n",
    "    Returns a new dataframe with hashed CSN column\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df_hashed = df.copy()\n",
    "    \n",
    "    # Use a fixed salt for consistency\n",
    "    FIXED_SALT = \"your_fixed_salt_here\"  # You can change this value\n",
    "    \n",
    "    def hash_value(value):\n",
    "        if pd.isna(value):\n",
    "            return None\n",
    "        salted = f\"{str(value)}{FIXED_SALT}\".encode()\n",
    "        return hashlib.sha256(salted).hexdigest()[:12]\n",
    "    \n",
    "    # Apply the hash function to the CSN column\n",
    "    df_hashed['csn'] = df_hashed['csn'].apply(hash_value)\n",
    "    \n",
    "    return df_hashed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1897d86b-db43-4319-a7f0-ad1a4154dc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "with open('../../seed', 'r') as file:\n",
    "    seed = file.readline().strip()\n",
    "\n",
    "def shift_dates_inplace(df, seed, min_weeks=52, max_weeks=52*2):\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    \"\"\"Shift datetime columns in place\"\"\"\n",
    "    random.seed(seed)\n",
    "    weeks_to_add = random.randint(min_weeks, max_weeks)\n",
    "    shift_delta = timedelta(weeks=weeks_to_add)\n",
    "\n",
    "    datetime_cols = df_copy.select_dtypes(include=['datetime64[ns]', 'datetime64']).columns\n",
    "\n",
    "    for col in datetime_cols:\n",
    "        df_copy[col] = df_copy[col].apply(lambda x: x + shift_delta if pd.notna(x) else x)\n",
    "\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b728ef24-098e-4c6c-8349-c174f8d42f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# set date range\n",
    "arrived_after = '2024-01-01'\n",
    "arrived_before = '2024-01-31'\n",
    "\n",
    "# create parameters dictionary\n",
    "params = {\n",
    "    'arrived_after': arrived_after,\n",
    "    'arrived_before': arrived_before\n",
    "}\n",
    "\n",
    "# set up SQL query\n",
    "SQL_DIR = Path(\"/home/jovyan/work/zella/zbeds/sql\")\n",
    "subquery = (SQL_DIR / \"EMAP_ed_subquery.sql\").read_text()\n",
    "mainquery = (SQL_DIR / \"EMAP_test_script.sql\").read_text()\n",
    "final_query = mainquery.replace('[subquery]', f'({subquery})')\n",
    "\n",
    "# execute the combined query\n",
    "df = pd.read_sql(\n",
    "    final_query,\n",
    "    engine,\n",
    "    params=params\n",
    ")\n",
    "\n",
    "# Hash the csns before display\n",
    "df = hash_csn(df)\n",
    "\n",
    "# shift the dates before display\n",
    "df = shift_dates_inplace(df, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f42660-6444-464f-89c2-5a062e8552ba",
   "metadata": {},
   "source": [
    "## Identify moment of departure from ED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aada876-6bf2-4fca-96dd-31bfc7e43f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(['csn', 'location_arrival'], inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ce3bfc-7d8a-41bd-a752-72915603f4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a mask for ED locations using string operations\n",
    "ed_mask = (\n",
    "    df['location_string'].str.startswith('ED^') |\n",
    "    df['location_string'].str.startswith('1020100166^') |\n",
    "    df['location_string'].str.startswith('1020100170^')\n",
    ")\n",
    "\n",
    "# Filter for ED locations and group by CSN to find first departure\n",
    "first_ed_departure = (\n",
    "    df[ed_mask]\n",
    "    .groupby('csn')['location_departure']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'location_departure': 'first_ed_departure'})\n",
    ")\n",
    "\n",
    "# merge this back with original dataframe:\n",
    "df_with_departure = df.merge(\n",
    "    first_ed_departure,\n",
    "    on='csn',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eeae078-5040-45fb-80dd-332a3bea0559",
   "metadata": {},
   "source": [
    "## Identify whether patient was admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc452e2-491e-4c85-a0a6-9b477f71e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate admission status\n",
    "def determine_admission(group):\n",
    "    # Get rows after first ED departure\n",
    "    post_ed = group[group['location_arrival'] >= group['first_ed_departure']]\n",
    "    return len(post_ed) > 0\n",
    "\n",
    "# Group by CSN and apply the admission check\n",
    "admissions = (\n",
    "    df_with_departure\n",
    "    .groupby('csn')\n",
    "    .apply(determine_admission)\n",
    "    .reset_index()\n",
    "    .rename(columns={0: 'is_admitted'})\n",
    ")\n",
    "\n",
    "# Merge admission status back to dataframe\n",
    "df_with_admission_status = df_with_departure.merge(\n",
    "    admissions,\n",
    "    on='csn',\n",
    "    how='left'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ca1c96-28e8-4a8f-a143-a90d11b245ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_admission_status.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a8ff5-bebf-4423-a7f8-6bd32bfda6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify final_df to include only ED rows\n",
    "\n",
    "df_final = df_with_admission_status[df_with_admission_status.location_departure <= df_with_admission_status.first_ed_departure]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b6995f-f95f-4eb3-b2cc-b35252b5e19e",
   "metadata": {},
   "source": [
    "## Set up an array of snapshot datetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c8b81a-7773-4997-8a45-5c1806dbb5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate whether the notebook is being run locally for UCLH or with public datasets\n",
    "uclh = False\n",
    "from patientflow.load import set_file_paths\n",
    "from patientflow.load import load_config_file\n",
    "\n",
    "# set file locations\n",
    "data_folder_name = 'data-uclh' if uclh else 'data-public'\n",
    "data_file_path, media_file_path, model_file_path, config_path = set_file_paths(\n",
    "        train_dttm = None, data_folder_name = data_folder_name, uclh = uclh, from_notebook=True, inference_time = False)\n",
    "\n",
    "# load params\n",
    "params = load_config_file(config_path)\n",
    "\n",
    "snapshot_times = params[\"prediction_times\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a779f7-0a28-403e-be0b-7a7fd9c4f2f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time, timedelta\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def get_shifted_snapshot_dates(arrived_after, arrived_before, seed, min_weeks=52, max_weeks=52*2):\n",
    "    # First get the original dates\n",
    "    original_dates = pd.date_range(\n",
    "        start=arrived_after, \n",
    "        end=arrived_before, \n",
    "        freq=\"D\"\n",
    "    ).date.tolist()[:-1]\n",
    "    \n",
    "    # Apply the same shift\n",
    "    random.seed(seed)\n",
    "    weeks_to_add = random.randint(min_weeks, max_weeks)\n",
    "    shift_delta = timedelta(weeks=weeks_to_add)\n",
    "    \n",
    "    # Shift each date\n",
    "    shifted_dates = [date + shift_delta for date in original_dates]\n",
    "    \n",
    "    return shifted_dates\n",
    "\n",
    "snapshot_dates = get_shifted_snapshot_dates(arrived_after, arrived_before, seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24354460-e824-4978-ba1e-c6c54d2d639b",
   "metadata": {},
   "source": [
    "## Create snapshots dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa1d953-2022-4392-97cf-193dde9cc913",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, time\n",
    "import pandas as pd\n",
    "\n",
    "def create_snapshots(df, snapshot_times, snapshot_dates):\n",
    "    # Create empty list to store all results\n",
    "    all_results = []\n",
    "    \n",
    "    # For each combination of date and time\n",
    "    for date in snapshot_dates:\n",
    "        for hour, minute in snapshot_times:\n",
    "            snapshot_datetime = datetime.combine(\n",
    "                date, \n",
    "                time(hour=hour, minute=minute)\n",
    "            )\n",
    "            \n",
    "            # Filter dataframe for this snapshot\n",
    "            mask = (df['location_arrival'] <= snapshot_datetime) & (df['location_departure'] > snapshot_datetime)\n",
    "            snapshot_df = df[mask].copy()  # Create copy to avoid SettingWithCopyWarning\n",
    "            \n",
    "            # Add snapshot information columns\n",
    "            snapshot_df['snapshot_date'] = date\n",
    "            snapshot_df['snapshot_time'] = [(hour, minute)] * len(snapshot_df)\n",
    "            \n",
    "            # Append to results list\n",
    "            all_results.append(snapshot_df)\n",
    "    \n",
    "    # Combine all results into single dataframe\n",
    "    if all_results:\n",
    "        final_df = pd.concat(all_results, ignore_index=True)\n",
    "        snapshot_cols = ['snapshot_date', 'snapshot_time']\n",
    "        other_cols = [col for col in final_df.columns if col not in snapshot_cols]\n",
    "        final_df = final_df[snapshot_cols + other_cols]\n",
    "    else:\n",
    "        # Create empty dataframe with correct columns if no results found\n",
    "        final_df = pd.DataFrame(columns=list(df.columns) + ['snapshot_date', 'snapshot_time', 'snapshot_datetime'])\n",
    "    \n",
    "    return final_df.drop(columns = ['patient_class', 'presentation_datetime', 'hospital_arrival', 'hospital_departure', 'location_arrival', 'location_departure', 'first_ed_departure'])\n",
    "\n",
    "snapshots_df = create_snapshots(df_final, snapshot_times, snapshot_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6ed379-8e7c-4e31-ac62-909fdc5b7773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doesn't appear in snapshots because this patient was whizzed to the stroke unit\n",
    "snapshots_df[snapshots_df.csn=='000639d6912b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e83d703-ce58-4e87-bee6-4878a7cd33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots_df#.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbc857e-a2e2-4fd9-9767-f9d2da13019a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
